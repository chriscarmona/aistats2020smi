\documentclass{article}

%\usepackage{arxiv}
% \usepackage{neurips_2019}
\usepackage[preprint]{neurips_2019}
%\usepackage[final]{neurips_2019}

\input{preamble}

\title{Semi-Modular Inference \\ Supplementary material}

% \author{
% Author 1 \\
% Department 1\\
% City 1\\
% email 1\\
% \And
% Author 2 \\
% Department 2\\
% City 2\\
% email 2\\
% }

\author{
Chris U. Carmona \\
Department of Statistics\\
University of Oxford\\
Oxford, U \\
\texttt{carmona@stats.ox.ac.uk}\\
\And
Geoff K. Nicholls \\
Department of Statistics\\
University of Oxford\\
Oxford, UK \\
\texttt{nicholls@stats.ox.ac.uk}\\
}


<<init_chunk, eval=TRUE, echo=FALSE>>=
rm(list = ls())
options(scipen=999, stringsAsFactors=FALSE)
set.seed(0)

# Indicates if the MCMC will be computed (TRUE), or loaded from previously computed results (FALSE)
compute_mcmc = FALSE

# loading required packages #
req.pck <- c( "tidyverse","foreach","doParallel","doRNG","cowplot",
              "abind","mvtnorm" )
req.pck <- sapply(X=req.pck,FUN=require,character.only=T)
if(!all(req.pck)) {
  sapply(X=req.pck[!req.pck],FUN=install.packages,character.only=T);
  sapply(X=req.pck,FUN=require,character.only=T)
}

# # Parallel processing
# parallel_comp = TRUE
# if(parallel_comp){
#   n_cores = 25
#   options(cores=n_cores)
#   doParallel::registerDoParallel()
#   getDoParWorkers()
# }

# auxilliar functions used in foreach loops #
lrcomb <- function(...) { mapply('rbind', ..., SIMPLIFY=FALSE) }
lacomb <- function(...) { mapply('abind', ..., MoreArgs=list(along=3),SIMPLIFY=FALSE) }
acomb <- function(...) {abind(..., along=3)}

@

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

In this document we present additional material which complements the main article. The numbers of sections in this document is aligned with the sections in the main article, for the ease of referencing (this does not apply for subsections and lower levels).


\textbf{Notation: } For the sake of compactness in our derivations, the following expressions are used:
\begin{align*}
p(Z) &= \int \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi\\
p(Z,Y)_{\theta} &= \int \int p(Z \mid \varphi) \; p( Y \mid \varphi, \theta ) \; p(\varphi,\theta) \; d\varphi \; d\theta\\
p(Z,Y_{\eta})_{\tilde\theta} &= \int \int p(Z \mid \varphi) \; p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta\\
p(Y_{\eta})_{\tilde\theta} &= \int \int p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta\\
p(Y,\varphi)_{\theta} &= \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta \\
p(Y  \mid  \varphi)_{\theta} &= \frac{1}{p(\varphi)} \; p(Y , \varphi)_{\theta}\\
\end{align*}

All the figures and numerical results presented in the main text and this supplementcan be replicated using our R package, \texttt{aistats2020smi}, available in Github.
<<load_aistats2020smi, eval=TRUE, echo=TRUE>>=
# devtools::install_github("christianu7/aistats2020smi")
library(aistats2020smi)
@

\setcounter{section}{1}

\section{Background methods}\label{sec:background}

% Geoff comments on misspecification
Here are a few remarks on model misspecification. Classically, misspecification is identified in goodness-of-fit checks as poor posterior predictive performance on held out data. In this paper, a model is relatively more misspecified if it has relatively worse performance in posterior predictive checks. This is defined in more detail in Section~\ref{sec:opt_eta}. Notice that this may be caused by a misspecified observation model, as is the case in the example in Sections~\ref{sec:hpv_analysis}, but unrepresentative prior assumptions may also lead to mispecification. This is illustrated in Section~\ref{sec:biased_data}. The example in Section~\ref{sec:agric_analysis} arguably suffers from both forms of mispecification.

\subsection{Explicit formulae for cut posterior}

The graphical model analysed in the main text is shown in Figure~\ref{fig:toy_multimodular_model}.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
    % Nodes
    \node (Z) [data] {$Z$};
    \node (Y) [data, right=of Z, xshift=0.5cm] {$Y$};
    \node (phi) [param, below=of Z] {$\varphi$};
    \node (theta) [param, below=of Y] {$\theta$};
    % Edges
    \edge {phi} {Z,Y};
    \edge {theta} {Y};
    % SMI line
    \draw[dashed,red,line width=0.5mm] (0.85,0) to (0.85,-1.25);
    % Text denoting modules
    \node[text width=3cm] at (0.5,1) {Module 1};
    \node[text width=3cm] at (2.75,1) {Module 2};
\end{tikzpicture}
\end{center}
\caption{Graphical representation of a simple multi-modular model.}
\label{fig:toy_multimodular_model}
\end{figure}

The \textbf{conventional (full) posterior} for this model is:
\begin{align} \label{eq:full_post}
 p(\varphi,\theta \mid Z,Y) &= p(\varphi \mid Z, Y) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= p(Z,Y,\varphi,\theta ) \; \frac{ 1 }{ p(Z,Y)_{\theta} } \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \theta ) \; p(\varphi, \theta) \; \frac{ 1 }{ p(Z,Y)_{\theta} }
\end{align}

The \textbf{cut posterior} for this model is defined \citep{Plummer2015} as:
\begin{align} \label{eq:cut_post}
 p_{cut}(\varphi,\theta \mid Z,Y) &= p(\varphi \mid Z) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= \frac{ p(Z \mid \varphi) p(\varphi) }{ p(Z) } \frac{ p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) }{ p( Y,\varphi)_{\theta}} \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \theta ) \; \frac{ 1 }{ p(Z)} \; \frac{ 1 }{p(Y \mid \varphi)_{\theta} }  p(\varphi, \theta)
\end{align}

Note the relation between the cut posterior and the conventional posterior
\begin{align*}
 p_{cut}(\varphi,\theta \mid Z,Y) &= p(Z, Y , \varphi, \theta ) \; \frac{ 1 }{ p(Z) \; p(Y \mid \varphi)_{\theta} } \\
 &= p(\varphi, \theta \mid Z, Y ) \; \frac{ p(Z, Y) }{ p(Z) \; p(Y \mid \varphi)_{\theta} } \\
\end{align*}

\section{Semi-Modular Inference} \label{sec:smi}

\subsection{Explicit formulae for SMI posterior}


The \textbf{$\eta$-smi posterior} is defined as
\begin{align} \label{eq:smi_post}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) &= p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= \frac{ p(Z \mid \varphi) \; p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) }{ p(Z, Y)_{\tilde\theta}} \frac{ p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) }{ p( Y , \varphi)_{\theta} } \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &\propto p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

In the penultimate step, we assume that $\theta$ and $\tilde\theta$ are conditionally independent given $\varphi$ in the prior, so $ p(\varphi, \theta, \tilde\theta) = p(\theta \mid \varphi) p(\tilde\theta\mid \varphi) p(\varphi)= p( \varphi, \theta ) \; p( \varphi, \tilde\theta ) \frac{1}{p(\varphi)} $.

From here is easy to see two particular cases of the $\eta$-smi posterior: the cut model $p_{smi,\eta}(\varphi,\theta \mid Z,Y) = p_{cut}(\varphi,\theta \mid Z,Y)$ when $\eta=0$; and the conventional posterior
 $p_{smi,\eta}(\varphi,\theta \mid Z,Y) = p(\varphi,\theta \mid Z,Y)$ when $\eta=1$.

\section{Analysis with (Semi-)Modular Inference} \label{sec:smi_considerations}

\subsection{Coherent update of beliefs}
In \cite{Bissiri2016}, the conventional update of beliefs provided by Bayes theorem is expanded, providing a generalised framework in which alternative inferential options are justified beyond the conventional posterior.

The framework to some extent relies on the idea that an update of beliefs must exist. This update of beliefs is performed, under a decision theory framework, by a function $\psi$, which turns the prior into posterior beliefs by incorporating new observed data $y$ via a loss function $l(\theta;y)$, that is
\begin{equation*}
  p(\theta \mid y) = \psi\{ l(\theta;y) , p(\theta) \}
\end{equation*}

Such update $\psi$ is \textbf{Coherent} if it ensures that we end up with the same posterior, whether we update our beliefs by observing all data simultaneously or by observing the data sequentially,
\begin{equation*}
  \psi\{ l(\theta;x_2) , \psi\{ l(\theta;x_1) , p(\theta) \} \} = \psi\{ l(\theta;x_1)+l(\theta;x_2) , p(\theta) \}
\end{equation*}

They show that an optimal, valid and coherent update of beliefs is of the form
\begin{equation*}
	p(\theta \mid y ) = \psi\{ l(\theta;y) , p(\theta) \} = \frac{ \exp\{ -l(\theta;y) \} \; p(\theta) }{\int \exp\{ -l(\theta;y) \} \; p(\theta) \; d\theta}
\end{equation*}

The flexibility of the framework stablished by \cite{Bissiri2016} allows us to analyse the SMI posterior and the cut model posterior as alternative updates of beliefs. From Eq.~\ref{eq:smi_post} we can see that the loss function underlying the SMI posterior is
\begin{equation} \label{eq:smi_loss}
  l_{smi,\eta}( (\varphi,\theta,\tilde\theta) ; (Z,Y) ) = - \log p(Z \mid \varphi) - \eta \log p(Y \mid \varphi,\tilde\theta) -\log p(Y \mid \varphi,\theta) + \log p(Y \mid \varphi) .
\end{equation}
Similarly, from Eq.~\ref{eq:cut_post} we can see that the loss function for the cut model is
\begin{equation} \label{eq:cut_loss}
  l_{cut}( (\varphi,\theta) ; (Z,Y) ) = - \log p(Z \mid \varphi) -\log p(Y \mid \varphi,\theta) + \log p(Y \mid \varphi).
\end{equation}


\subsection{Coherence of the SMI posterior} \label{sec:suppl_cut_consistency}
Here we demonstrate that the smi posterior preserve multi-modular coherence. In multi-modular settings, coherence must hold in two ways: 1) by observing responses from different modules one after the other (i.e. first $Z$, and then $Y$); and 2) by observing sequential fragments within the same module (e.g. first $Z_1$, and then $Z_2$, with $Z=(Z_1,Z_2)$).

The SMI posterior in Eq.~\ref{eq:smi_post} updates the belief distribution by observing the two datasets simultaneously. Nevertheless, we can also update by observing data only from one module at a time, and still preserve loss function in eq.\ref{eq:smi_loss}.

Say our current distribution of beliefs about $(\varphi,\theta, \tilde\theta)$ is $p(\varphi,\theta, \tilde\theta)$. Our updated belief by observing \textit{only} $Z$ would be
\begin{align} \label{eq:smi_post_z}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z) &= \psi\{ l( (\varphi,\theta,\tilde\theta); Z ) , p( \varphi, \theta,\tilde\theta) \} \nonumber \\
 &= p(Z \mid \varphi) \; \frac{1}{ p(Z ) } p(\varphi, \theta, \tilde\theta) \\
 &= p(Z \mid \varphi) \frac{ 1 }{ \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

similarly, if we observe \textit{only} $Y$ our updated beliefs are
\begin{align} \label{eq:smi_post_y}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Y) &= \psi\{ l( (\varphi,\theta,\tilde\theta); Y ) , p( \varphi, \theta,\tilde\theta) \} \nonumber \\
 &= p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p( Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &= p( Y \mid \varphi, \tilde \theta )^\eta \; p( Y \mid \varphi,  \theta ) \frac{ 1 }{ \int \int p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta} \frac{ p(\varphi) }{ \int  p( Y \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta} p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

\subsubsection{Coherence under sequential modules}

First, we show that the update from prior to posterior, is equivalent to updating sequentially first $Z$ and then $Y$. This is (a)=(b)+(c) in the following diagram:

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(a)", bend left=10] \ar[r,"(b)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z) \ar[r,"(c)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z,Y)
\end{tikzcd}

The update (a) is given by Equation~\ref{eq:smi_post}.

The update (b) is in Eq.~\ref{eq:smi_post_z}
\begin{align} \label{eq:smi_update_b}
 p_{(b)}(\varphi,\theta,\tilde\theta \mid Z) &= p(Z \mid \varphi) \; \frac{1}{ p(Z)} p(\varphi, \theta, \tilde\theta) \nonumber \\
 &= p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta).
\end{align}

The update (b)+(c) is equivalent to Eq.~\ref{eq:smi_post_y} substituting the current beliefs with $p_{(b)}(\varphi,\theta,\tilde\theta \mid Z)$
\begin{align*}
 p_{(b)+(c)}(\varphi,\theta,\tilde\theta \mid Z, Y) &\propto p( Y \mid \varphi, \tilde \theta )^\eta p( Y \mid \varphi,  \theta ) \frac{ p_{(b)}(\varphi \mid Z) }{ \int  p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta \mid Z ) \;  d\theta} p_{(b)}(\varphi, \theta, \tilde\theta \mid Z)\\
 &\propto p(Z \mid \varphi) p( Y \mid \varphi, \tilde \theta )^\eta p( Y \mid \varphi,  \theta ) \frac{ 1 }{ P(Y \mid \varphi )_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}

The equivalence $p_{(b)+(c)}(\varphi,\theta,\tilde\theta \mid Z, Y)=p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z,Y)$ is clear by comparing the last formula with smi posterior in Eq.~\ref{eq:smi_post}. For the last line, we used the following identity
\begin{align*}
 \frac{ p_{(b)}(\varphi \mid Z) }{ \int  p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta \mid Z ) \;  d\theta } &=\frac{ \int \int p_{(b)}(\varphi, \theta, \tilde\theta\mid Z) d\theta d\tilde\theta }{ \int  \int p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta, \tilde\theta\mid Z ) \;  d\theta d\tilde\theta } \\
 &= \frac{ \int \int p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta}{ \int \int p( Y \mid \varphi,  \theta ) \; p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) \;  d\theta d\tilde\theta } \\
 &=\frac{ p(\varphi ) }{ \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta} \\
 &= \frac{1}{p(Y \mid \varphi)_{\theta}}.
\end{align*}

Now, we want a similar result by first observing $Y$ and then $Z$, i.e. (a)=(d)+(e) in the following diagram

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(a)", bend left=10] \ar[r,"(d)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y) \ar[r,"(e)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z,Y)
\end{tikzcd}

The update (a) is again given by Equation~\ref{eq:smi_post}.

The update (d) is Eq.~\ref{eq:smi_post_y}
\begin{equation} \label{eq:smi_update_d}
 p_{(d)}(\varphi,\theta,\tilde\theta \mid Y) \propto p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{equation}

The update (d)+(e) is equivalent to Eq.~\ref{eq:smi_post_z} substituting the current beliefs with $p_{(d)}(\varphi,\theta,\tilde\theta \mid Y)$
\begin{align*}
 p_{(d)+(e)}(\varphi,\theta,\tilde\theta \mid Z, Y) &\propto p( Z \mid \varphi) \; p_{(d)}(\varphi, \theta, \tilde\theta \mid Y ) \\
 &= p( Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} \; p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}
The equivalence $p_{(d)+(e)}(\varphi,\theta,\tilde\theta \mid Z, Y)=p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z,Y)$ is direct from comparing the last formula with Eq.~\ref{eq:smi_post}.

\subsubsection{Coherence under data partition from the same module}

Now we verify that the smi posterior is coherent when observing a sequential portions of the same module. Define the partitions $Z=(Z_1,Z_2)$ and $Y=(Y_1,Y_2)$.

First, we verify coherence for the partition of data $Z$. We want to check (b)=(b1)+(b2) in the following diagram.
\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(b)", bend left=10] \ar[r,"(b1)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z_1) \ar[r,"(b2)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z)
\end{tikzcd}

Update (b) is the same as defined above in Equation~\ref{eq:smi_update_b}

Updates (b1) and (b2) are similar to Eq.~\ref{eq:smi_post_z}, substituting the corresponding $Z$ and current state of beliefs
\begin{align*}
 p_{(b1)}(\varphi,\theta,\tilde\theta \mid Z_1) &= p(Z_1 \mid \varphi) \; \frac{1}{  \int p(Z_1 \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta), \\
 p_{(b1)+(b2)}(\varphi,\theta,\tilde\theta \mid Z_1,Z_2) &= p(Z_2 \mid \varphi) \; \frac{1}{  \int p_{(b1)}(Z_2 \mid \varphi) \; p_{(b1)}(\varphi \mid Z_1) \; d\varphi } p_{(b1)}(\varphi, \theta, \tilde\theta \mid Z_1) \\
 &\propto p(Z_1 \mid \varphi) \; p(Z_2 \mid \varphi) \; p(\varphi, \theta, \tilde\theta ) \\
 &= p(Z \mid \varphi) \; p(\varphi, \theta, \tilde\theta ),
\end{align*}
clearly $p_{(b1)+(b2)}(\varphi,\theta,\tilde\theta \mid Z_1,Z_2)=p_{(b)}(\varphi,\theta,\tilde\theta \mid Z)$.

Lastly, we verify coherence for the partition of data $Y$. We want to check (d)=(d1)+(d2) in the following diagram.

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(d)", bend left=10] \ar[r,"(d1)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y_1) \ar[r,"(d2)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y)
\end{tikzcd}

Update (d) is the same as defined above in Equation~\ref{eq:smi_update_d}

Updates (d1) and (d2) are similar to Eq.~\ref{eq:smi_post_y}, substituting the corresponding $Y$ and current state of beliefs
\begin{align*}
 p_{(d1)}(\varphi,\theta,\tilde\theta \mid Y_1) = & p(Y_1 \mid \varphi, \tilde\theta )^{\eta} p(Y_1 \mid \varphi, \theta ) \; \frac{1}{ p( Y_{1 ,\eta} )_{\tilde\theta} \; p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 \propto & p(Y_1 \mid \varphi, \tilde\theta )^{\eta} p(Y_1 \mid \varphi, \theta ) \; \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 = & p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{ p(\varphi) }{ \int  p( Y_1 \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta} p(\varphi, \theta, \tilde\theta) \nonumber \\
 p_{(d1)+(d2)}(\varphi,\theta,\tilde\theta \mid Y_1,Y_2) \propto & p( Y_2 \mid \varphi, \tilde \theta )^\eta \; p( Y_2 \mid \varphi,  \theta ) \frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta} p_{(d1)}(\varphi, \theta, \tilde\theta) \nonumber \\
\propto & \left( p(Y_1 \mid \varphi, \tilde\theta ) \; p(Y_2 \mid \varphi, \tilde\theta ) \right)^{\eta} \left( p(Y_1 \mid \varphi, \theta ) \; p(Y_2 \mid \varphi, \theta ) \right) \cdot \\
 & \cdot \frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta} \frac{ 1 }{ p( Y_1 \mid \varphi )_{\theta} } p(\varphi,\theta,\tilde\theta) \\
\propto & p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}
from here is clear that $p_{(d1)+(d2)}(\varphi,\theta,\tilde\theta \mid Y_1,Y_2)=p_{(d)}(\varphi,\theta,\tilde\theta \mid Y)$. In the last step, we used the following identity
\begin{align*}
\frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta}  &= \frac{ \int \int p_{(d1)}(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta }{ \int \int p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta, \tilde\theta ) \;  d\theta d\tilde\theta}\\
&\propto \frac{ \int \int p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta }{ \int \int p( Y_2 \mid \varphi,  \theta ) \; p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \;  d\theta d\tilde\theta} \\
&= \frac{ \frac{1}{p(\varphi)} \left( \int p( Y_1 \mid \varphi, \tilde \theta )^\eta p(\varphi, \tilde\theta) d\tilde\theta \right) \left(\int  p( Y_1 \mid \varphi,  \theta ) p(\varphi, \theta) d\theta \right) }{ \frac{1}{p(\varphi)} \left( \int  p( Y_1 \mid \varphi, \tilde \theta )^\eta p(\varphi, \tilde\theta) \;  d\tilde\theta \right) \left(  \int p( Y_1 \mid \varphi,  \theta ) p( Y_2 \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta \right) }\\
&=\frac{ p(Y_1 \mid \varphi)_{\theta} }{ p(Y \mid \varphi)_{\theta} }
\end{align*}
here again, we assumed that $\theta$ and $\tilde\theta$ are conditionally independent given $\varphi$ in the prior, so $ p(\varphi, \theta, \tilde\theta) = p( \varphi, \theta ) \; p( \varphi, \tilde\theta ) \frac{1}{p(\varphi)} $.

\subsection{Detailed balance of SMI posterior}

Here we show that the $\eta$-smi posterior (and cut posterior in particular) preserves detail balance with the transition kernel implied by a two-stage update using two conditional distributions

\begin{enumerate}
    \item Sample $(\varphi,\tilde\theta) \sim p_{\eta-pow}(\varphi, \tilde\theta \mid Z, Y) = p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} } p(\varphi, \tilde\theta) $
    \item Sample $\theta \sim p( \theta \mid Y,\varphi )=p(Y \mid \varphi, \theta ) \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\theta) $
\end{enumerate}

The first step updates $(\varphi,\tilde\theta)$ using the powered likelihood. It is not difficult to target this posterior using traditional sampling methods.

The second term updates $\theta$ exactly from its conditional posterior given data $Y$ from module 2, and a \textit{fixed} value $\varphi$.

The transition kernel for one iteration in this scheme is given by
\begin{equation} \label{eq:smi_kernel}
K(\varphi',\theta',\tilde\theta' \mid \varphi,\theta,\tilde\theta)= K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) p(\theta' \mid Y,\varphi').
\end{equation}

By construction, the first stage of the update (using the powered likelihood) is in detailed balance with the powered likelihood, i.e. satisfies
\begin{equation*}
p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y) K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) = p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) K(\varphi,\tilde\theta \mid \varphi',\tilde\theta')
\end{equation*}

From here we see that the $\eta$-smi posterior in Eq.~\ref{eq:smi_post} satisfies detailed balance with the transition kernel in eq.
\ref{eq:smi_kernel}

\begin{align*}
p_{smi,\eta}&(\varphi,\theta,\tilde\theta \mid Z,Y) K(\varphi',\theta',\tilde\theta' \mid \varphi,\theta,\tilde\theta) \\
&= [ p_{\eta-pow}(\varphi, \tilde\theta \mid Z,Y)p(\theta \mid Y,\varphi) ][ K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) p(\theta' \mid Y,\varphi') ] \\
&= p_{\eta-pow}(\varphi, \tilde\theta \mid Z,Y) p(\theta \mid Y,\varphi) \frac{p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) K(\varphi,\tilde\theta \mid \varphi',\tilde\theta')}{p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y)} p(\theta' \mid Y,\varphi')\\
&= p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) p(\theta' \mid Y,\varphi') K(\varphi,\tilde\theta \mid \varphi',\tilde\theta') p(\theta \mid Y,\varphi)\\
&=p_{smi,\eta}(\varphi',\theta',\tilde\theta' \mid Z,Y) K(\varphi,\theta,\tilde\theta \mid \varphi',\theta',\tilde\theta')
\end{align*}


\subsection{Scalability of MCMC. Big-O runtime and bottlenecks}

Our baseline is standard Bayes-MCMC on the original full model. We suppose for simplicity this was implemented using separate updates for $\theta|\varphi$ and $\varphi|\theta$, though these need not be Gibbs updates.
Let $\tau_{\varphi,\theta}$ be the Integrated Autocorrelation Time (IACT) of Bayes-MCMC. If the Effective Sample Size (ESS) of the full Bayes-MCMC output is $N$ then we must have done $T=N\tau_{\varphi,\theta}$ MCMC steps. If {\it one Bayes-MCMC step updating both $\theta$ and $\varphi$ has unit cost} then the overall cost is $W_{bm}=T$[time]. This doesn't parallelise.

The SMI posterior is \[
p_{\eta-smi}(\varphi,\tilde\theta,\theta|Y,Z)=p_{\eta-pow}(\varphi,\tilde\theta|Y,Z)p(\theta|Y,\varphi).
\]
If we use the same Bayes-MCMC updates to sample $p_{\eta-pow}(\varphi,\tilde\theta|Y,Z)$ then the work sampling $(\varphi,\tilde\theta)$ at one $\eta$-value is $W_{Bayes}$. Thin the $\varphi$ samples every $\tau_{\varphi,\theta}$ steps to get an ESS about $N$ (this is rough, because the target changes with $\eta$, but reasonable if we allow for some tuning of the MCMC with $\eta$ - we didnt need to tune in our examples).

We use the Bayes-MCMC update for $p(\theta|Y,\varphi)$ in SMI. Let $\tau_\theta$ be the IACT. Typically, $\tau_\theta<\tau_{\varphi,\theta}$ (the target has lower dimension; illustrative proofs can be given in simple special cases) so take $\tau_\theta=\tau_{\varphi,\theta}$ (conservative, and note that these ``side-chains'' targeting $p(\theta|Y,\varphi)$ can be started close to equilibrium using the $\theta$-value output from sampling at the previous step).
We run the $\theta$-sampler to equilibrium (initialise $\theta^{(t)}$-run with $\theta^{(t-1)}$). Suppose this takes $K\tau_\theta$ steps ($K\approx 5$ is reasonable).

In the following we assume simulation at different $\eta$-values is parallelized over machines, while simulation of $\theta|\phi$ is parallelized over threads on a machine. Suppose we have $M_{t}$ threads on each of $M_{p}$ machines. The $\theta$-sampling parallelizes with a small communication overhead if the time to do $K\tau_{\varphi,\theta}$ of the $\theta|\varphi$-updates is significantly larger than the communication time. 
The cost of the $\theta$-update in the second stage of SMI is no more than the cost of one update in the original Bayes-MCMC where both $\theta$ and $\varphi$ were updated, so a runtime cost for $\theta$-updates equal one unit is conservative. It follows that $(\tilde\theta,\varphi,\theta)$-sampling at one $\eta$-value costs about \[
W_{\eta}=W_{bm}(1+K/M_{t}).
\]

We have to repeat this $J$ times, sampling $p_{\eta-smi}(\varphi,\tilde\theta,\theta|Y,Z)$ for each of $J$ different $\eta$-values spaced between $\eta=0$ and $\eta=1$ ($J\approx 20$ should be enough) using $M_{p}$ machines. This larger task parallelises essentially perfectly. The total cost is
\[
W_{smi}=W_{bm}(1+K/M_{t}) \times J/M_{p}.
\]
For eg if we assign resources as $M_{p}=J$ and $M_{t}=1$ (just parallelise over $\eta$) the SMI cost is not worse than about $10$ times the cost of doing Bayes-MCMC. This reflects our experience.

Finally, we compute and smooth the WAIC across the $J$ runs at different $\eta$-values. This part is fast output-analysis. The ESS must be big enough to get stable WAIC estimates, but WAIC is "nice" to estimate. Very roughly, 
\[W_{smi}\simeq 10 W_{bm}\] 
should be achievable without a great deal of work on top of the cost of implementing and running standard Bayes-MCMC.



\section{Data Analyses} \label{sec:data_analyses}

\subsection{Simulation study: Biased data} \label{sec:biased_data}

\begin{multicols}{2}
Model:
\begin{align*}
  Z \mid \varphi &\sim N( \varphi, \sigma_z^2 ) \\
  Y \mid \varphi, \theta  &\sim N( \varphi + \theta, \sigma_y^2 )
  % \text{with } & \sigma_z^2, \sigma_y^2 \text{ known}
\end{align*}
with $\sigma_z^2$ and $\sigma_y^2$ (and other $\sigma$'s) known.

Priors:
\begin{align*}
  \varphi &\sim N( 0, \sigma_\varphi^2 ) \\
  \theta &\sim N( 0, \sigma_\theta^2 ) \\
  \tilde\theta &\sim N( 0, \tilde\sigma_\theta^2 )
\end{align*}
\end{multicols}

In our example on the main text, we know the generative parameters: $(\varphi^*,\theta^*, \tilde\theta^*)$. We compare these \textit{true} values with the estimates arising from the $\eta$-smi posterior, for different values of $\eta\in[0,1]$.

% \begin{align*}
%   p( Z \mid \varphi ) &= (2\pi\sigma_z^2)^{-n/2} \exp\{ -\frac{1}{2\sigma_z^2} \sum_{i=1}^n (z_i-\varphi)^2 \} \\
%   p( Y \mid \varphi,  \theta ) &= (2\pi\sigma_y^2)^{-m/2} \exp\{ -\frac{1}{2\sigma_y^2} \sum_{i=1}^m (y_i-\varphi-\theta)^2 \} \\
%   p( \varphi ) &= (2\pi\sigma_\varphi^2)^{-1/2} \exp\{ -\frac{1}{2\sigma_\varphi^2} \varphi^2 \} \\
%   p( \theta ) &= (2\pi\sigma_\theta^2)^{-1/2} \exp\{ -\frac{1}{2\sigma_\theta^2} \theta^2 \} \\
% \end{align*}

\subsubsection{SMI posterior}

First, derive $p(Y \mid \varphi)_{\theta}$ as a function of $\varphi$
\begin{align*}
  p(Y \mid \varphi)_{\theta} &= \frac{1}{p(\varphi)} \; \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta \\
  &\propto \exp\{ -\frac{1}{2} [ \varphi^2 ( \frac{m}{m \sigma_\theta^2 + \sigma_y^2 } ) - 2 \varphi (\bar Y \frac{m}{m \sigma_\theta^2 + \sigma_y^2 }) ] \}
\end{align*}

Now we can obtain the $\eta$-smi posterior
\begin{align*}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) &= p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &\propto \exp\{ -\frac{1}{2} \; [ \varphi^2 ( \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) + \frac{1}{\sigma_\varphi^2} - \frac{m}{\sigma_y^2 + m \sigma_\theta^2} ) - 2 \varphi (\frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \bar Y \frac{m}{ \sigma_y^2 + m \sigma_\theta^2}) + \\
 & \hspace{2cm} \theta^2 (\frac{m}{\sigma_y^2} + \frac{1}{\sigma_\theta^2}) - 2 \theta (\bar Y \frac{m}{\sigma_y^2 }) + \\
 & \hspace{2cm} \tilde\theta^2 ( \eta \frac{m}{\sigma_y^2} + \frac{1}{\tilde\sigma_\theta^2}) - 2 \tilde\theta (\eta \bar Y \frac{m}{\sigma_y^2 }) + \\
 & \hspace{2cm} + 2 \varphi \theta (\frac{m}{\sigma_y^2}) + 2 \varphi \tilde\theta (\eta \frac{m}{\sigma_y^2 }) ] \}
\end{align*}

From here we see that the joint posterior distribution for $(\varphi,\theta,\tilde\theta)$ is a multivariate normal distribution defined as:
\begin{equation} \label{eq:smi_post_5_1}
p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) = \text{Normal}( \mu, \Sigma ),
\end{equation}

with
\begin{equation*}
  \Sigma = \begin{bmatrix}
  \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \frac{m}{ \sigma_y^2 + m \sigma_\theta^2 } + \frac{1}{\sigma_\varphi^2} & \frac{m}{\sigma_y^2} & \eta \frac{m}{\sigma_y^2 } \\
  \frac{m}{\sigma_y^2} & \frac{m}{\sigma_y^2} + \frac{1}{\sigma_\theta^2} & 0 \\
  \eta \frac{m}{\sigma_y^2 } & 0 &  \eta \frac{m}{\sigma_y^2} + \frac{1}{\tilde\sigma_\theta^2}
\end{bmatrix}^{-1} \text{, and } \mu = \Sigma \begin{bmatrix}
  \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \bar Y \frac{m}{ \sigma_y^2 + m \sigma_\theta^2} \\
  \bar Y \frac{m}{\sigma_y^2 } \\
  \eta \bar Y \frac{m}{\sigma_y^2 }
\end{bmatrix}.
\end{equation*}

The generative parameters described in the main text are as follows
<<biased_data_01, eval=TRUE, echo=TRUE>>=
n=25 # Sample size for Z
m=50 # Sample size for Y

phi = 0 
theta = 1 # bias

sigma_z = 2 # variance for Z
sigma_y = 1 # variance for Y
@

The true bias is $\theta=1$. Assume we have an over-optimistic view of the bias, with prior distribution centered in 0 and relatively small prior variance.
<<biased_data_02, eval=TRUE, echo=FALSE>>=
sigma_phi=Inf # Prior variance for phi
sigma_theta=0.5 # Prior variance for eta

param_names = c('phi','theta','theta_tilde')
param_true = c(phi,theta,theta)

# sequence of eta values in (0,1)
eta_all = seq(0,1,0.01)
@

<<biased_data_03, eval=TRUE, echo=FALSE>>=
set.seed(123)
Z = rnorm( n=n, mean=phi, sd=sigma_z)
Y = rnorm( n=m, mean=phi+theta, sd=sigma_y )
# cat('Z_mean=',mean(Z),'; Y_mean=',mean(Y))

post_eta_all = foreach::foreach(eta = eta_all,.combine='lrcomb', .multicombine=TRUE) %dopar% {
  # eta = 0.01
  # Compute posterior mean and variance
  posterior = aistats2020smi::SMI_post_biased_data( Z=Z, Y=Y, sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta )
  list( t(posterior[[1]]), diag(posterior[[2]]) )
}
# Compute MSE
mse_eta_all = post_eta_all[[2]] + ( post_eta_all[[1]] - t( matrix(param_true,3,length(eta_all)) ) )^2

# Plot posterior vs true value
aistats2020smi::set_ggtheme()
post_plot_all = foreach::foreach( par_i = seq_along(param_names) ) %do% {
  
  p = data.frame( eta=eta_all,
                  post_mean=post_eta_all[[1]][,par_i],
                  post_sd=post_eta_all[[2]][,par_i]^0.5 ) %>%
    ggplot() + 
    geom_line( aes(x=eta,y=post_mean), col='red' ) +
    geom_line( aes(x=eta,y=post_mean+post_sd), col='blue', lty=3 ) +
    geom_line( aes(x=eta,y=post_mean-post_sd), col='blue', lty=3 ) +
    geom_hline(yintercept=param_true[par_i], lty=2) +
    labs(y=paste(param_names[par_i]," posterior",sep=""))
  p
}
p = cowplot::plot_grid(  post_plot_all[[1]], post_plot_all[[2]], post_plot_all[[3]], ncol=1, align='v' )
ggsave( plot=p,
          filename="SMI_biased_posterior_single_dataset.pdf",
          device="pdf", width=10,height=15, units="cm")
@

In Figure~\ref{fig:smi_post_5_1} we show posterior distributions (mean $\pm$ std. dev.) for a randomly generated dataset ($\bar Z=$\Sexpr{round(mean(Z),4)}; $\bar Y=$\Sexpr{round(mean(Y),4)}) using the generative parameters described in the main text. Note that the conventional bayes ($\eta=1$) is the worst estimation for the true parameters $\varphi$ accross all posible candidates $\eta \in [0,1]$.
<<biased_data_04, eval=TRUE, echo=TRUE>>=
# Posterior for conventional bayes eta=1
posterior = SMI_post_biased_data( Z=Z, Y=Y,
                                  sigma_z=sigma_z, sigma_y=sigma_y,
                                  sigma_phi=sigma_phi,
                                  sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta,
                                  eta=1 )
posterior = mapply('rownames<-', posterior, MoreArgs=list(value=param_names))
# posterior mean
posterior$mean
@

\begin{figure}[!ht]
\center
\includegraphics[width=0.5\textwidth]{SMI_biased_posterior_single_dataset}
\caption{Posterior distribution of $\varphi$, $\theta$ and $\tilde\theta$. A black horizontal line shows the true generative value. Mean (solid red line) $\pm$ one std. dev. (dotted blue line) }
  \label{fig:smi_post_5_1}
\end{figure}

\subsubsection{Mean Square Error (MSE)}

From Equation~\ref{eq:smi_post}, we can compute the MSE of estimates arising from the SMI posterior $\varphi$, $\theta$ and , $\tilde\theta$,
\begin{align*}
  MSE(\varphi) &= \Sigma_{[1,1]} + (\mu_{[1]}-\varphi^*)^2\\
  MSE(\theta) &= \Sigma_{[2,2]} + (\mu_{[2]}-\theta^*)^2\\
  MSE(\tilde\theta) &= \Sigma_{[3,3]} + (\mu_{[3]}-\theta^*)^2
\end{align*}

In the first simulation study of the main text we display $MSE(\varphi)$ and $MSE(\theta)$ to demonstrate that we can reach smaller MSE with values of $\eta$ other than 0 and 1. To generate this plots, we produced 1000 synthetic datasets, computed MSE using Eq.~\ref{eq:smi_post} on each one, with a grid of values of $\eta\in[0,1]$. The MSE lines displayed correspond to the \textit{average} MSE across datasets, for each value of $\eta$.

<<biased_data_05, eval=TRUE, echo=FALSE>>=
set.seed(123)
  
# Average Mean Square Error #

# Compute Posterior mean and sd for each iteration
n_iter = 10
Z = matrix(rnorm( n=n*n_iter, mean=phi, sd=sigma_z),n_iter,n)
Y = matrix(rnorm( n=m*n_iter, mean=phi+theta, sd=sigma_y ),n_iter,m)
post_eta_all_iter = foreach(iter_i = 1:n_iter,.combine='lacomb', .multicombine=TRUE)  %:%
  foreach(eta_i = seq_along(eta_all), .combine='lrcomb', .multicombine=TRUE) %dopar% {
    # eta_i=1
    posterior = aistats2020smi::SMI_post_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    list( t(posterior[[1]]), diag(posterior[[2]]) )
}
# Compute MSE for each iteration
param_true_array = aperm( array(param_true,dim=c(3,length(eta_all),n_iter)) , c(2,1,3) )
MSE_all_iter = post_eta_all_iter[[2]] + (post_eta_all_iter[[1]]-param_true_array)^2

# Average across iterations
post_eta_all_average = list( apply(post_eta_all_iter[[1]],c(1,2),mean),
                             apply(post_eta_all_iter[[2]],c(1,2),mean) + apply(post_eta_all_iter[[1]],c(1,2),var))
MSE_average = apply(MSE_all_iter,c(1,2),mean)

# Plot MSE
aistats2020smi::set_ggtheme()
mse_plot_all = foreach::foreach( par_i = seq_along(param_names) ) %do% {
  # par_i=1
  p = data.frame( eta=eta_all,
                  mse=MSE_average[,par_i] ) %>%
    ggplot() + 
    geom_line( aes(x=eta,y=mse), col='red' ) +
    labs(y=paste("MSE( ",param_names[par_i]," )",sep=""))
  p
}
p = cowplot::plot_grid(  mse_plot_all[[1]], mse_plot_all[[2]], ncol=1, align='v' )
ggsave( plot=p,
        filename="SMI_biased_MSE_average.pdf",
        device="pdf", width=10,height=10, units="cm")

# MSE theta vs theta_tilde
aistats2020smi::set_ggtheme()
p = MSE_average %>%
  `colnames<-`(param_names) %>%
  as.data.frame() %>%
  mutate(eta=eta_all) %>%
  tidyr::pivot_longer(cols=all_of(param_names),names_to='parameter') %>%
  dplyr::filter(parameter %in% c('theta','theta_tilde')) %>%
  ggplot() +
  geom_line( aes(x=eta,y=value,col=parameter) ) +
  coord_cartesian(ylim=c(0.28,0.50)) +
  labs(y="MSE( theta )")
ggsave( plot=p,
        filename="SMI_biased_MSE_average_theta.pdf",
        device="pdf", width=10,height=10, units="cm")
@

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_MSE_average.pdf}
  \caption{Mean Squared Error of the two main parameters under SMI posterior.}
  \label{fig:SMI_biased_MSE_average}
\end{figure}

Here we also show a comparison between $\theta$ and $\tilde\theta$. Figure~\ref{fig:SMI_biased_MSE_average} shows that $\tilde\theta$ is dominated by $\theta$ in MSE. The comparison emphasize the convenience of SMI over power likelihood discussed in section 4.4 of the main text.

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_MSE_average_theta.pdf}
  \caption{Comparison of the bias estimation under SMI (theta) vs powered likelihood (theta tilde)}
  \label{fig:SMI_biased_MSE_average_theta}
\end{figure}

\subsubsection{Expected log pointwise predictive density (elpd)}

The simplicity and conjugacy of this model allow us to derive exact formula for the elpd
\begin{equation*}
  elpd = \int\int p^*(z,y) \log p_{smi,\eta}( z, y \mid Z,Y) dz dy
\end{equation*}
where $p^*$ is the distribution representing the true data-generating process and
\begin{equation*}
  p_{smi,\eta}(z,y \mid Z,Y)=\int\int p(z,y \mid \varphi, \theta) \; p_{smi,\eta}(\varphi,\theta \mid Y,Z)\, d\varphi\,d\theta
\end{equation*}
is a candidate posterior predictive distribution, indexed by $\eta$.

Let $\begin{bmatrix}  a & b \\ b & c \end{bmatrix}=Cov(\varphi,\theta \mid Z,Y)^{-1}$ be the  inverse of the posterior covariance matrix of $(\varphi,\theta)$, and $\begin{bmatrix}  d \\ e \end{bmatrix}=E(\varphi,\theta \mid Z,Y)$ the posterior means.

Following straightforward Gaussian completion we can show that the joint posterior distribution for $\varphi$, $\theta$, and new data $z_0$ and $y_0$ is:
\begin{align*}
  p_{smi,\eta}(z_0,y_0,\varphi,\theta|Z,Y) &\propto p(z_0,y_0|\varphi,\theta) \; p_{smi,\eta}(\varphi,\theta|Z,Y) \\
  &\propto \exp\{ -\frac{1}{2} \; [ z_0^2 (\frac{1}{\sigma_z^2}) + y_0^2 (\frac{1}{\sigma_y^2}) + \varphi^2 (a+\frac{1}{\sigma_z^2}+\frac{1}{\sigma_y^2}) + \theta^2 (c + \frac{1}{\sigma_y^2} ) + \\
  & \hspace{2cm} - 2 z_0 \varphi (\frac{1}{\sigma_z^2}) - 2 y_0 \varphi (\frac{1}{\sigma_y^2}) - 2 y_0 \theta (\frac{1}{\sigma_y^2}) + 2 \varphi \theta (b+\frac{1}{\sigma_y^2}) \\
  & \hspace{2cm} -2 \varphi (a d + b e) -2 \theta (b d + c e ) ] \}
\end{align*}

So we have
\begin{equation} \label{eq:smi_post_pred_5_1}
p_{smi,\eta}(z_0,y_0,\varphi,\theta \mid Z, Y ) = \text{Normal}( \mu, \Sigma ),
\end{equation}

with
\begin{equation*}
  \Sigma = \begin{bmatrix}
  \frac{1}{\sigma_z^2} & 0 & -\frac{1}{\sigma_z^2} & 0 \\
  0 & \frac{1}{\sigma_y^2} & -\frac{1}{\sigma_y^2} & -\frac{1}{\sigma_y^2} \\
  -\frac{1}{\sigma_z^2} & -\frac{1}{\sigma_y^2} & a+\frac{1}{\sigma_z^2}+\frac{1}{\sigma_y^2} & b+\frac{1}{\sigma_y^2} \\
  0 & -\frac{1}{\sigma_y^2} & b+\frac{1}{\sigma_y^2} & c + \frac{1}{\sigma_y^2}
\end{bmatrix}^{-1} \text{, and } \mu = \Sigma \begin{bmatrix}
  0 \\
  0 \\
  a d + b e \\
  b d + c e
\end{bmatrix}.
\end{equation*}

We know the true generative values $\varphi^*$ and $\theta^*$, so we can compute $elpd$ using Monte Carlo samples from the true generative distribution $p^*$ and evaluate this values in the log-density of the bivariate normal $(z_0,y_0|Z,Y)$ from Equation~\ref{eq:smi_post_pred_5_1}.

<<biased_data_06, eval=TRUE, echo=FALSE>>=
# ELPD approximation via Monte Carlo
set.seed(0)
n_new = 10

# generate data from the ground-truth distribution
Z_new = matrix( rnorm( n=n_iter*n_new, mean=phi, sd=sigma_z), n_iter, n_new )
Y_new = matrix( rnorm( n=n_iter*n_new, mean=phi+theta, sd=sigma_y), n_iter, n_new )

log_pred_eta_all_iter = foreach(iter_i = 1:n_iter, .combine='acomb', .multicombine=TRUE) %:%
  foreach( eta_i = seq_along(eta_all), .combine=rbind ) %dopar% {
    # iter_i=1
    # new_i = 1
    # eta_i=1
    # posterior = aistats2020smi::SMI_post_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    predictive = aistats2020smi::SMI_pred_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    mvtnorm::dmvnorm( x=cbind(Z_new[iter_i,],Y_new[iter_i,]), mean=predictive[[1]] , sigma=predictive[[2]], log=TRUE )
}
# average to get elpd
elpd_eta_all = apply(log_pred_eta_all_iter,1,mean)

# Plot ELPD
aistats2020smi::set_ggtheme()
p = data.frame(eta=eta_all, elpd=elpd_eta_all) %>%
  ggplot() +
  geom_line( aes(x=eta,y=-elpd),col='red' ) +
  labs(y="- elpd(z,y)")
ggsave( plot=p,
          filename="SMI_biased_elpd.pdf",
          device="pdf", width=10,height=7, units="cm")
@

In Figure~\ref{fig:SMI_biased_elpd} we show the Monte Carlo estimation of the $elpd$. We select the optimal $\eta$ as the value that maximise the $elpd$. To generate this plot, we produced 1000 synthetic datasets, computed $elpd$ on each one (using Monte Carlo), with a grid of values of $\eta\in[0,1]$. The $elpd$ line correspond to the \textit{average} elpd across datasets, for each value of $\eta$.

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_elpd.pdf}
  \caption{ELPD under SMI posterior.}
  \label{fig:SMI_biased_elpd}
\end{figure}

\subsection{Experiments on Bayesian Multiple Imputation}\label{sec:bayes_impute}

Bla

\subsection{Agricultural practices in early civilizations} \label{sec:agric_analysis}

The aim of the study, described in detail in \cite{Styring2017}, is to provide statistical evidence about a specific agricultural practice of the first urban centres in northern Mesopotamia.

The hypothesis is that increased agricultural production to support growing urban populations was achieved by cultivation of larger areas of land, entailing lower manure/midden inputs per unit area. This practice is known as \emph{extensification}.

Our contribution goes into extending the methods used to perform Bayesian analysis in this adverse scenario of model misspecification and big missing data.

\subsubsection{ Data }

The data consists of measurements of nitrogen and carbon isotopes for a collection of crop remains. There are two datasets: archaeological and modern, which we will denote by $\mathcal{A}$ and $\mathcal{M}$, respectively. First, the \emph{Archaeological} dataset $\mathcal{A}$, consists of data gathered from excavations of antique crop sites in the region of Mesopotamia. Second, the \emph{Modern} dataset, $\mathcal{M}$, was gathered in a controlled experimental setting in recent years. Further characteristics and description of variables on each dataset can be found in the statistical supplement to \cite{Styring2017}.

\subsubsection{ The model } \label{sec:agric_model}

We preserved the model stated in \cite{Styring2017}. The main goal of this study is to is to provide statistical evidence for the hypothesis of \emph{extensification} in the ancient urban sites. This hypothesis can be condensed in analysing the strength of the effect of the site size $S_i$ on the corresponding manure level $M_i$ for the records in the archaeological data, $\{i \in \mathcal{A}\}$. The more negative the estimated effect, the stronger the evidence is supporting the extensification hypothesis.

However, there is no available information about the Manuring levels in the ancient dataset. This is addressed by using a \emph{Data augmentation} perspective, and consider that all the corresponding values of the manure level in the Archaeological data are missing.

The model consists of a two-module model, which integrates archaeological and calibration data so the missing manuring levels can be inferred. The first module is a Proportional Odds model ($PO$), with the missing Manure Levels in the archaeological data as the ordinal response ($M_i; i \in \mathcal{A}$). The second module consists of a linear Gaussian model ($HM$), applicable to both datasets, with the Nitrogen level of the crops ($Z$) as the response, and Manure levels as one of the predcitors. The graphical representation of the model is depicted in Figure~\ref{fig:agricurb_model}

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[thick,scale=1, every node/.style={transform shape}]
  % initial node: M_arc at (0,0)
  % Manure Level in archaeological data
  \node (M_arc) [param] {$M_i$};

  % Paramaters in PO
  \node (gamma) [param, fill=red!30, above=of M_arc, yshift=1.5cm, xshift=0.2cm] {$\gamma$};
  \node (alpha) [param, left=of gamma, xshift=0.2cm] {$\alpha$};
  \node (xi) [param, left=of alpha, xshift=0.2cm] {$\xi_s$};
  \node (sigma_xi) [param, left=of xi] {$\sigma_{\xi}$};
  \plate {po_rndeff} {(xi)} {\tiny $s \in S_{\mathcal{A}}$} ;

  % Archaeological data in PO
  \node (S_arc_po) [data, fill=blue!20, left=of M_arc, xshift=-0.5cm] {$S_i$};
  \node (P_arc_po) [data, above=of S_arc_po] {$P_i$};
  \plate {arc_po} {(P_arc_po) (S_arc_po) (M_arc)} {$i \in \mathcal{A}$};

  % Archaeological data in HM
  \node (C_arc_hm) [data, right=of M_arc] {$C_i$};
	\node (P_arc_hm) [data, right=of C_arc_hm] {$P_i$};
	\node (R_arc_hm) [param, below=of P_arc_hm] {$R_i$};
  \node (Z_arc_hm) [data, below=of C_arc_hm, yshift=-1cm] {$Z_i$};
  \plate {arc_hm} {(M_arc) (C_arc_hm) (P_arc_hm) (R_arc_hm) (Z_arc_hm)} {$i \in \mathcal{A}$};

  % Paramaters in HM
  \node (beta) [param, right=of R_arc_hm, xshift=0.2cm] {$\beta$};
  \node (sigma) [param, below=of beta, yshift=0.2cm] {$\sigma$};
	\node (v) [param, below=of sigma, yshift=0.2cm] {$v$};
  \node (zeta) [param, below=of v, yshift=0.2cm] {$\zeta_s$};
	\node (sigma_zeta) [param, right=of zeta] {$\sigma_{\zeta}$};
  \plate {hm_rndeff} {(zeta)} {\tiny $s \in S_{\mathcal{A} \cup \mathcal{M}}$};

  % modern data in HM
  \node (M_mod_hm) [data, right=of P_arc_hm, xshift=1.6cm] {$M_j$};
  \node (C_mod_hm) [data, right=of M_mod_hm ] {$C_j$};
	\node (P_mod_hm) [data, right=of C_mod_hm] {$P_j$};
	\node (R_mod_hm) [data, below=of P_mod_hm] {$R_j$};
  \node (Z_mod_hm) [data, below=of C_mod_hm, yshift=-1cm] {$Z_j$};
  \plate {mod_hm} {(M_mod_hm) (C_mod_hm) (P_mod_hm) (R_mod_hm) (Z_mod_hm)} {$j \in \mathcal{M}$};;

  % PO model plate
  \plate [dotted,label={ \large PO module}]{po} {(M_arc) (alpha) (gamma) (xi) (sigma_xi)  (S_arc_po) (P_arc_po) (po_rndeff) (arc_po) } {}
  % HM model plate
	\plate [dotted, label={ \large HM module}]{hm} {(M_arc) (C_arc_hm) (P_arc_hm) (R_arc_hm) (Z_arc_hm) (arc_hm) (M_mod_hm) (C_mod_hm) (P_mod_hm) (R_mod_hm) (Z_mod_hm) (mod_hm) (beta) (sigma) (v) (zeta) (sigma_zeta) (hm_rndeff) } {}

  % SMI line
	\draw[dashed,red, line width=0.5mm] (-1.1,-1.2) to[out=90, in=180, distance=2cm] (1.1,1.3);

  % arrows
  % PO parameters
  \edge {sigma_xi} {xi};
  \edge {alpha, gamma, xi} {M_arc};
  % PO data
  \edge {S_arc_po, P_arc_po} {M_arc};

  % HM parameters
  \edge {sigma_zeta} {zeta};
  \edge {beta,zeta,sigma,v} {Z_arc_hm};
  \edge {beta,zeta,sigma,v} {Z_mod_hm};
  % HM data
	\edge {M_arc, C_arc_hm, P_arc_hm, R_arc_hm} {Z_arc_hm};
  \edge {M_mod_hm, C_mod_hm, P_mod_hm, R_mod_hm} {Z_mod_hm};
  
\end{tikzpicture}
\end{center}
\caption[Graphical model for agricultural data]{ Graphical representation of the model for the agricultural data. Squares denote observable variables and circles denote unknown quantities (parameters and missing data).The main interest of the study is on the parameter $\gamma$ (red circle), effect of size $S_i$ (blue square) on Manure level $M_i$. The dashed line indicates the cut where SMI is applied for the imputation of missing manure.}
\label{fig:agricurb_model}
\end{figure}

The model can be studied from a multi-modular perspective. Indeed, the supplement of \cite{Styring2017} performs Bayesian Multiple Imputation (BMI) to impute the missing manure levels with parameters learnt from the $HM$ module, and therefore \emph{cutting} the influence from the $PO$ module into this imputation. The parameters of the $PO$ module are then infered conditional on the imputed values and other information in the archaelogical data.

In Figure~\ref{fig:agricurb_model_simple} we draw a simplified version of the complete model to clarify how the SMI framework can be applied in this setting. The mapping of variables between the complicated and simple graphs is as follows: the missing manure levels will take the role of $\phi$; the observed data in the $HM$ module is $Z$, the parameters in the $PO$ module will be $\theta$, and the rest of arcahaeological data in the $PO$ module is $Y$.

\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}[thick,scale=1, every node/.style={transform shape}]
  \node (theta) [param] {$\theta$};
  \node (varphi) [param, right=of theta, xshift=0.5cm] {$\varphi$};
  \node (Y) [data, below=of theta] {$Y$};
  \node (Z) [data, below=of varphi] {$Z$};

  % SMI line
	\draw[dashed, red, line width=0.5mm] (0.85,0.5) to (0.85,-1.25);

  % arrows
  % PO parameters
  \edge {theta, Y} {varphi};
  \edge {varphi} {Z};
  
\end{tikzpicture}
\end{center}
\caption{ Simplified representation of the model for the agricultural data.}
\label{fig:agricurb_model_simple}
\end{figure}

The simplified version resembles our graphical model in Figure~\ref{fig:toy_multimodular_model}. From here it is more clear how the Bayesian imputation approach took by \cite{Styring2017} is equivalent to a cut model: first learning $\phi$ from the $HM$ module, and then learning $\theta$ conditional on $\phi$ and $Y$. This scheme yields our known cut posterior from Eq.~\ref{eq:cut_post}.
\[P_{cut}(\varphi,\theta \mid Z,Y) = P(\varphi \mid Z) \; P(\theta \mid Y,\varphi) \]

\subsection{Epidemiological study: HPV virus and cervical Cancer} \label{sec:hpv_analysis}
Bla

\bibliographystyle{apalike} %use the apalike bibliography style
\bibliography{Mendeley} % Mendeley bibliography

\end{document}
