\documentclass{article}

%\usepackage{arxiv}
% \usepackage{neurips_2019}
\usepackage[preprint]{neurips_2019}
%\usepackage[final]{neurips_2019}

\input{preamble}

\title{Semi-Modular Inference \\ Supplementary material}

% \author{
% Author 1 \\
% Department 1\\
% City 1\\
% email 1\\
% \And
% Author 2 \\
% Department 2\\
% City 2\\
% email 2\\
% }

\author{
Chris U. Carmona \\
Department of Statistics\\
University of Oxford\\
Oxford, U \\
\texttt{carmona@stats.ox.ac.uk}\\
\And
Geoff K. Nicholls \\
Department of Statistics\\
University of Oxford\\
Oxford, UK \\
\texttt{nicholls@stats.ox.ac.uk}\\
}


<<init_chunk, eval=TRUE, echo=FALSE>>=
rm(list = ls())
options(scipen=999, stringsAsFactors=FALSE)
set.seed(0)

# Indicates if the MCMC will be computed (TRUE), or loaded from previously computed results (FALSE)
compute_mcmc = FALSE

# loading required packages #
req.pck <- c( "tidyverse","foreach","doParallel","doRNG","cowplot",
              "abind","mvtnorm" )
req.pck <- sapply(X=req.pck,FUN=require,character.only=T)
if(!all(req.pck)) {
  sapply(X=req.pck[!req.pck],FUN=install.packages,character.only=T);
  sapply(X=req.pck,FUN=require,character.only=T)
}

# # Parallel processing
# parallel_comp = TRUE
# if(parallel_comp){
#   n_cores = 25
#   options(cores=n_cores)
#   doParallel::registerDoParallel()
#   getDoParWorkers()
# }

# auxilliar functions used in foreach loops #
lrcomb <- function(...) { mapply('rbind', ..., SIMPLIFY=FALSE) }
lacomb <- function(...) { mapply('abind', ..., MoreArgs=list(along=3),SIMPLIFY=FALSE) }
acomb <- function(...) {abind(..., along=3)}

@

\begin{document}
\SweaveOpts{concordance=TRUE}
\maketitle

In this document we present additional material which complements the main article. The numbers of sections in this document is aligned with the sections in the main article, for the ease of referencing (this does not apply for subsections and lower levels).


\textbf{Notation: } For the sake of compactness in our derivations, the following expressions are used:
\begin{align*}
p(Z) &= \int \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi\\
p(Z,Y)_{\theta} &= \int \int p(Z \mid \varphi) \; p( Y \mid \varphi, \theta ) \; p(\varphi,\theta) \; d\varphi \; d\theta\\
p(Z,Y_{\eta})_{\tilde\theta} &= \int \int p(Z \mid \varphi) \; p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta\\
p(Y_{\eta})_{\tilde\theta} &= \int \int p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta\\
p(Y,\varphi)_{\theta} &= \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta \\
p(Y  \mid  \varphi)_{\theta} &= \frac{1}{p(\varphi)} \; p(Y , \varphi)_{\theta}\\
\end{align*}

All the figures and numerical results presented in the main text and this supplementcan be replicated using our R package, \texttt{aistats2020smi}, available in Github.
<<load_aistats2020smi, eval=TRUE, echo=TRUE>>=
# devtools::install_github("christianu7/aistats2020smi")
library(aistats2020smi)
@

\setcounter{section}{1}

\section{Background methods}\label{sec:background}

% Geoff comments on misspecification
What is model misspecification? Classically, misspecification is identified in goodness-of-fit checks as poor posterior predictive performance on held out data. In this paper, a model is relatively more misspecified if it has relatively worse performance in posterior predictive checks. This is defined in more detail in Section~\ref{sec:opt_eta}. Notice that this may be caused by a misspecified observation model, as is the case in the example in Sections~\ref{sec:hpv_analysis}, but unrepresentative prior assumptions may also lead to mispecification. This is illustrated in Section~\ref{sec:biased_data}. The example in Section~\ref{sec:agric_analysis} arguably suffers from both forms of mispecification.

\subsection{Explicit formulae for cut posterior}

Recall the graphical analysed in the main text
\begin{figure}[!ht]
\begin{center}
\begin{tikzpicture}
    \node (Z) [data] {$Z$};
    \node (Y) [data, right=of Z] {$Y$};
    \node (phi) [param, below=of Z] {$\varphi$};
    \node (theta) [param, below=of Y] {$\theta$};

    \draw[dashed,red,line width=0.5mm] (1,0) -- (1,-2);

    \edge {phi} {Z,Y};
    \edge {theta} {Y};

    \node[text width=3cm] at (0,1.2) {Module 1};
    \node[text width=3cm] at (3.5,1.2) {Module 2};
\end{tikzpicture}
\end{center}
\caption{Graphical representation of a simple multi-modular model.}
\label{fig:toy_multimodular_model}
\end{figure}

The \textbf{conventional (full) posterior} for this model is:
\begin{align} \label{eq:full_post}
 p(\varphi,\theta \mid Z,Y) &= p(\varphi \mid Z, Y) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= p(Z,Y,\varphi,\theta ) \; \frac{ 1 }{ p(Z,Y)_{\theta} } \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \theta ) \; p(\varphi, \theta) \; \frac{ 1 }{ p(Z,Y)_{\theta} }
\end{align}



The \textbf{cut posterior} for this model is defined \citep{Plummer2015} as:
\begin{align} \label{eq:cut_post}
 p_{cut}(\varphi,\theta \mid Z,Y) &= p(\varphi \mid Z) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= \frac{ p(Z \mid \varphi) p(\varphi) }{ p(Z) } \frac{ p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) }{ p( Y,\varphi)_{\theta}} \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \theta ) \; \frac{ 1 }{ p(Z)} \; \frac{ 1 }{p(Y \mid \varphi)_{\theta} }  p(\varphi, \theta)
\end{align}

Note the relation between the cut posterior and the conventional posterior
\begin{align*}
 p_{cut}(\varphi,\theta \mid Z,Y) &= p(Z, Y , \varphi, \theta ) \; \frac{ 1 }{ p(Z) \; p(Y \mid \varphi)_{\theta} } \\
 &= p(\varphi, \theta \mid Z, Y ) \; \frac{ p(Z, Y) }{ p(Z) \; p(Y \mid \varphi)_{\theta} } \\
\end{align*}

\section{Semi-Modular Inference} \label{sec:smi}

\subsection{Explicit formulae for SMI posterior}


The \textbf{$\eta$-smi posterior} is defined as
\begin{align} \label{eq:smi_post}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) &= p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y) \; p(\theta \mid Y,\varphi) \nonumber \\
 &= \frac{ p(Z \mid \varphi) \; p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) }{ p(Z, Y)_{\tilde\theta}} \frac{ p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) }{ p( Y , \varphi)_{\theta} } \nonumber \\
 &= p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &\propto p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

In the penultimate step, we assume that $\theta$ and $\tilde\theta$ are conditionally independent given $\varphi$ in the prior, so $ p(\varphi, \theta, \tilde\theta) = p(\theta \mid \varphi) p(\tilde\theta\mid \varphi) p(\varphi)= p( \varphi, \theta ) \; p( \varphi, \tilde\theta ) \frac{1}{p(\varphi)} $.

From here is easy to see two particular cases of the $\eta$-smi posterior: the cut model $p_{smi,\eta}(\varphi,\theta \mid Z,Y) = p_{cut}(\varphi,\theta \mid Z,Y)$ when $\eta=0$; and the conventional posterior
 $p_{smi,\eta}(\varphi,\theta \mid Z,Y) = p(\varphi,\theta \mid Z,Y)$ when $\eta=1$.

\section{Analysis with (Semi-)Modular Inference} \label{sec:smi_considerations}

\subsection{Coherent update of beliefs}
In \cite{Bissiri2016}, the conventional update of beliefs provided by Bayes theorem is expanded, providing a generalised framework in which alternative inferential options are justified beyond the conventional posterior.

The framework to some extent relies on the idea that an update of beliefs must exist. This update of beliefs is performed, under a decision theory framework, by a function $\psi$, which turns the prior into posterior beliefs by incorporating new observed data $y$ via a loss function $l(\theta;y)$, that is
\begin{equation*}
  p(\theta \mid y) = \psi\{ l(\theta;y) , p(\theta) \}
\end{equation*}

Such update $\psi$ is \textbf{Coherent} if it ensures that we end up with the same posterior, whether we update our beliefs by observing all data simultaneously or by observing the data sequentially,
\begin{equation*}
  \psi\{ l(\theta;x_2) , \psi\{ l(\theta;x_1) , p(\theta) \} \} = \psi\{ l(\theta;x_1)+l(\theta;x_2) , p(\theta) \}
\end{equation*}

They show that an optimal, valid and coherent update of beliefs is of the form
\begin{equation*}
	p(\theta \mid y ) = \psi\{ l(\theta;y) , p(\theta) \} = \frac{ \exp\{ -l(\theta;y) \} \; p(\theta) }{\int \exp\{ -l(\theta;y) \} \; p(\theta) \; d\theta}
\end{equation*}

The flexibility of the framework stablished by \cite{Bissiri2016} allows us to analyse the SMI posterior and the cut model posterior as alternative updates of beliefs. From Eq.~\ref{eq:smi_post} we can see that the loss function underlying the SMI posterior is
\begin{equation} \label{eq:smi_loss}
  l_{smi,\eta}( (\varphi,\theta,\tilde\theta) ; (Z,Y) ) = - \log p(Z \mid \varphi) - \eta \log p(Y \mid \varphi,\tilde\theta) -\log p(Y \mid \varphi,\theta) + \log p(Y \mid \varphi) .
\end{equation}
Similarly, from Eq.~\ref{eq:cut_post} we can see that the loss function for the cut model is
\begin{equation} \label{eq:cut_loss}
  l_{cut}( (\varphi,\theta) ; (Z,Y) ) = - \log p(Z \mid \varphi) -\log p(Y \mid \varphi,\theta) + \log p(Y \mid \varphi).
\end{equation}


\subsection{Coherence of the SMI posterior} \label{sec:suppl_cut_consistency}
Here we demonstrate that the smi posterior preserve multi-modular coherence. In multi-modular settings, coherence must hold in two ways: 1) by observing responses from different modules one after the other (i.e. first $Z$, and then $Y$); and 2) by observing sequential fragments within the same module (e.g. first $Z_1$, and then $Z_2$, with $Z=(Z_1,Z_2)$).

The SMI posterior in Eq.~\ref{eq:smi_post} updates the belief distribution by observing the two datasets simultaneously. Nevertheless, we can also update by observing data only from one module at a time, and still preserve loss function in eq.\ref{eq:smi_loss}.

Say our current distribution of beliefs about $(\varphi,\theta, \tilde\theta)$ is $p(\varphi,\theta, \tilde\theta)$. Our updated belief by observing \textit{only} $Z$ would be
\begin{align} \label{eq:smi_post_z}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z) &= \psi\{ l( (\varphi,\theta,\tilde\theta); Z ) , p( \varphi, \theta,\tilde\theta) \} \nonumber \\
 &= p(Z \mid \varphi) \; \frac{1}{ p(Z ) } p(\varphi, \theta, \tilde\theta) \\
 &= p(Z \mid \varphi) \frac{ 1 }{ \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

similarly, if we observe \textit{only} $Y$ our updated beliefs are
\begin{align} \label{eq:smi_post_y}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Y) &= \psi\{ l( (\varphi,\theta,\tilde\theta); Y ) , p( \varphi, \theta,\tilde\theta) \} \nonumber \\
 &= p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p( Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &= p( Y \mid \varphi, \tilde \theta )^\eta \; p( Y \mid \varphi,  \theta ) \frac{ 1 }{ \int \int p( Y \mid \varphi, \tilde \theta )^\eta \; p(\varphi,\tilde\theta) \; d\varphi \; d\tilde\theta} \frac{ p(\varphi) }{ \int  p( Y \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta} p(\varphi, \theta, \tilde\theta) \nonumber
\end{align}

\subsubsection{Coherence under sequential modules}

First, we show that the update from prior to posterior, is equivalent to updating sequentially first $Z$ and then $Y$. This is (a)=(b)+(c) in the following diagram:

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(a)", bend left=10] \ar[r,"(b)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z) \ar[r,"(c)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z,Y)
\end{tikzcd}

The update (a) is given by Equation~\ref{eq:smi_post}.

The update (b) is in Eq.~\ref{eq:smi_post_z}
\begin{align} \label{eq:smi_update_b}
 p_{(b)}(\varphi,\theta,\tilde\theta \mid Z) &= p(Z \mid \varphi) \; \frac{1}{ p(Z)} p(\varphi, \theta, \tilde\theta) \nonumber \\
 &= p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta).
\end{align}

The update (b)+(c) is equivalent to Eq.~\ref{eq:smi_post_y} substituting the current beliefs with $p_{(b)}(\varphi,\theta,\tilde\theta \mid Z)$
\begin{align*}
 p_{(b)+(c)}(\varphi,\theta,\tilde\theta \mid Z, Y) &\propto p( Y \mid \varphi, \tilde \theta )^\eta p( Y \mid \varphi,  \theta ) \frac{ p_{(b)}(\varphi \mid Z) }{ \int  p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta \mid Z ) \;  d\theta} p_{(b)}(\varphi, \theta, \tilde\theta \mid Z)\\
 &\propto p(Z \mid \varphi) p( Y \mid \varphi, \tilde \theta )^\eta p( Y \mid \varphi,  \theta ) \frac{ 1 }{ P(Y \mid \varphi )_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}

The equivalence $p_{(b)+(c)}(\varphi,\theta,\tilde\theta \mid Z, Y)=p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z,Y)$ is clear by comparing the last formula with smi posterior in Eq.~\ref{eq:smi_post}. For the last line, we used the following identity
\begin{align*}
 \frac{ p_{(b)}(\varphi \mid Z) }{ \int  p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta \mid Z ) \;  d\theta } &=\frac{ \int \int p_{(b)}(\varphi, \theta, \tilde\theta\mid Z) d\theta d\tilde\theta }{ \int  \int p( Y \mid \varphi,  \theta ) \; p_{(b)}(\varphi, \theta, \tilde\theta\mid Z ) \;  d\theta d\tilde\theta } \\
 &= \frac{ \int \int p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta}{ \int \int p( Y \mid \varphi,  \theta ) \; p(Z \mid \varphi) \; \frac{1}{  \int p(Z \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta) \;  d\theta d\tilde\theta } \\
 &=\frac{ p(\varphi ) }{ \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta} \\
 &= \frac{1}{p(Y \mid \varphi)_{\theta}}.
\end{align*}

Now, we want a similar result by first observing $Y$ and then $Z$, i.e. (a)=(d)+(e) in the following diagram

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(a)", bend left=10] \ar[r,"(d)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y) \ar[r,"(e)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z,Y)
\end{tikzcd}

The update (a) is again given by Equation~\ref{eq:smi_post}.

The update (d) is Eq.~\ref{eq:smi_post_y}
\begin{equation} \label{eq:smi_update_d}
 p_{(d)}(\varphi,\theta,\tilde\theta \mid Y) \propto p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{equation}

The update (d)+(e) is equivalent to Eq.~\ref{eq:smi_post_z} substituting the current beliefs with $p_{(d)}(\varphi,\theta,\tilde\theta \mid Y)$
\begin{align*}
 p_{(d)+(e)}(\varphi,\theta,\tilde\theta \mid Z, Y) &\propto p( Z \mid \varphi) \; p_{(d)}(\varphi, \theta, \tilde\theta \mid Y ) \\
 &= p( Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} \; p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}
The equivalence $p_{(d)+(e)}(\varphi,\theta,\tilde\theta \mid Z, Y)=p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z,Y)$ is direct from comparing the last formula with Eq.~\ref{eq:smi_post}.

\subsubsection{Coherence under data partition from the same module}

Now we verify that the smi posterior is coherent when observing a sequential portions of the same module. Define the partitions $Z=(Z_1,Z_2)$ and $Y=(Y_1,Y_2)$.

First, we verify coherence for the partition of data $Z$. We want to check (b)=(b1)+(b2) in the following diagram.
\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(b)", bend left=10] \ar[r,"(b1)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z_1) \ar[r,"(b2)", swap] & p_{smi,\eta}(\varphi,\theta \mid Z)
\end{tikzcd}

Update (b) is the same as defined above in Equation~\ref{eq:smi_update_b}

Updates (b1) and (b2) are similar to Eq.~\ref{eq:smi_post_z}, substituting the corresponding $Z$ and current state of beliefs
\begin{align*}
 p_{(b1)}(\varphi,\theta,\tilde\theta \mid Z_1) &= p(Z_1 \mid \varphi) \; \frac{1}{  \int p(Z_1 \mid \varphi) \; p(\varphi) \; d\varphi } p(\varphi, \theta, \tilde\theta), \\
 p_{(b1)+(b2)}(\varphi,\theta,\tilde\theta \mid Z_1,Z_2) &= p(Z_2 \mid \varphi) \; \frac{1}{  \int p_{(b1)}(Z_2 \mid \varphi) \; p_{(b1)}(\varphi \mid Z_1) \; d\varphi } p_{(b1)}(\varphi, \theta, \tilde\theta \mid Z_1) \\
 &\propto p(Z_1 \mid \varphi) \; p(Z_2 \mid \varphi) \; p(\varphi, \theta, \tilde\theta ) \\
 &= p(Z \mid \varphi) \; p(\varphi, \theta, \tilde\theta ),
\end{align*}
clearly $p_{(b1)+(b2)}(\varphi,\theta,\tilde\theta \mid Z_1,Z_2)=p_{(b)}(\varphi,\theta,\tilde\theta \mid Z)$.

Lastly, we verify coherence for the partition of data $Y$. We want to check (d)=(d1)+(d2) in the following diagram.

\begin{tikzcd}
p(\varphi,\theta) \ar[rr,"(d)", bend left=10] \ar[r,"(d1)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y_1) \ar[r,"(d2)", swap] & p_{smi,\eta}(\varphi,\theta \mid Y)
\end{tikzcd}

Update (d) is the same as defined above in Equation~\ref{eq:smi_update_d}

Updates (d1) and (d2) are similar to Eq.~\ref{eq:smi_post_y}, substituting the corresponding $Y$ and current state of beliefs
\begin{align*}
 p_{(d1)}(\varphi,\theta,\tilde\theta \mid Y_1) = & p(Y_1 \mid \varphi, \tilde\theta )^{\eta} p(Y_1 \mid \varphi, \theta ) \; \frac{1}{ p( Y_{1 ,\eta} )_{\tilde\theta} \; p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 \propto & p(Y_1 \mid \varphi, \tilde\theta )^{\eta} p(Y_1 \mid \varphi, \theta ) \; \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 = & p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{ p(\varphi) }{ \int  p( Y_1 \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta} p(\varphi, \theta, \tilde\theta) \nonumber \\
 p_{(d1)+(d2)}(\varphi,\theta,\tilde\theta \mid Y_1,Y_2) \propto & p( Y_2 \mid \varphi, \tilde \theta )^\eta \; p( Y_2 \mid \varphi,  \theta ) \frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta} p_{(d1)}(\varphi, \theta, \tilde\theta) \nonumber \\
\propto & \left( p(Y_1 \mid \varphi, \tilde\theta ) \; p(Y_2 \mid \varphi, \tilde\theta ) \right)^{\eta} \left( p(Y_1 \mid \varphi, \theta ) \; p(Y_2 \mid \varphi, \theta ) \right) \cdot \\
 & \cdot \frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta} \frac{ 1 }{ p( Y_1 \mid \varphi )_{\theta} } p(\varphi,\theta,\tilde\theta) \\
\propto & p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta).
\end{align*}
from here is clear that $p_{(d1)+(d2)}(\varphi,\theta,\tilde\theta \mid Y_1,Y_2)=p_{(d)}(\varphi,\theta,\tilde\theta \mid Y)$. In the last step, we used the following identity
\begin{align*}
\frac{ p_{(d1)}(\varphi) }{ \int  p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta ) \;  d\theta}  &= \frac{ \int \int p_{(d1)}(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta }{ \int \int p( Y_2 \mid \varphi,  \theta ) \; p_{(d1)}(\varphi, \theta, \tilde\theta ) \;  d\theta d\tilde\theta}\\
&\propto \frac{ \int \int p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) d\theta d\tilde\theta }{ \int \int p( Y_2 \mid \varphi,  \theta ) \; p( Y_1 \mid \varphi, \tilde \theta )^\eta \; p( Y_1 \mid \varphi,  \theta ) \frac{1}{ p(Y_1 \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \;  d\theta d\tilde\theta} \\
&= \frac{ \frac{1}{p(\varphi)} \left( \int p( Y_1 \mid \varphi, \tilde \theta )^\eta p(\varphi, \tilde\theta) d\tilde\theta \right) \left(\int  p( Y_1 \mid \varphi,  \theta ) p(\varphi, \theta) d\theta \right) }{ \frac{1}{p(\varphi)} \left( \int  p( Y_1 \mid \varphi, \tilde \theta )^\eta p(\varphi, \tilde\theta) \;  d\tilde\theta \right) \left(  \int p( Y_1 \mid \varphi,  \theta ) p( Y_2 \mid \varphi,  \theta ) \; p(\varphi, \theta ) \;  d\theta \right) }\\
&=\frac{ p(Y_1 \mid \varphi)_{\theta} }{ p(Y \mid \varphi)_{\theta} }
\end{align*}
here again, we assumed that $\theta$ and $\tilde\theta$ are conditionally independent given $\varphi$ in the prior, so $ p(\varphi, \theta, \tilde\theta) = p( \varphi, \theta ) \; p( \varphi, \tilde\theta ) \frac{1}{p(\varphi)} $.

\subsection{Detailed balance of SMI posterior}

Here we show that the $\eta$-smi posterior (and cut posterior in particular) preserves detail balance with the transition kernel implied by a two-stage update using two conditional distributions

\begin{enumerate}
    \item Sample $(\varphi,\tilde\theta) \sim p_{\eta-pow}(\varphi, \tilde\theta \mid Z, Y) = p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} } p(\varphi, \tilde\theta) $
    \item Sample $\theta \sim p( \theta \mid Y,\varphi )=p(Y \mid \varphi, \theta ) \frac{1}{ p(Y \mid \varphi)_{\theta} } p(\theta) $
\end{enumerate}

The first step updates $(\varphi,\tilde\theta)$ using the powered likelihood. It is not difficult to target this posterior using traditional sampling methods.

The second term updates $\theta$ exactly from its conditional posterior given data $Y$ from module 2, and a \textit{fixed} value $\varphi$.

The transition kernel for one iteration in this scheme is given by
\begin{equation} \label{eq:smi_kernel}
K(\varphi',\theta',\tilde\theta' \mid \varphi,\theta,\tilde\theta)= K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) p(\theta' \mid Y,\varphi').
\end{equation}

By construction, the first stage of the update (using the powered likelihood) is in detailed balance with the powered likelihood, i.e. satisfies
\begin{equation*}
p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y) K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) = p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) K(\varphi,\tilde\theta \mid \varphi',\tilde\theta')
\end{equation*}

From here we see that the $\eta$-smi posterior in Eq.~\ref{eq:smi_post} satisfies detailed balance with the transition kernel in eq.
\ref{eq:smi_kernel}

\begin{align*}
p_{smi,\eta}&(\varphi,\theta,\tilde\theta \mid Z,Y) K(\varphi',\theta',\tilde\theta' \mid \varphi,\theta,\tilde\theta) \\
&= [ p_{\eta-pow}(\varphi, \tilde\theta \mid Z,Y)p(\theta \mid Y,\varphi) ][ K(\varphi',\tilde\theta' \mid \varphi,\tilde\theta) p(\theta' \mid Y,\varphi') ] \\
&= p_{\eta-pow}(\varphi, \tilde\theta \mid Z,Y) p(\theta \mid Y,\varphi) \frac{p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) K(\varphi,\tilde\theta \mid \varphi',\tilde\theta')}{p_{\eta-pow}(\varphi,\tilde\theta \mid Z,Y)} p(\theta' \mid Y,\varphi')\\
&= p_{\eta-pow}(\varphi',\tilde\theta' \mid Z,Y) p(\theta' \mid Y,\varphi') K(\varphi,\tilde\theta \mid \varphi',\tilde\theta') p(\theta \mid Y,\varphi)\\
&=p_{smi,\eta}(\varphi',\theta',\tilde\theta' \mid Z,Y) K(\varphi,\theta,\tilde\theta \mid \varphi',\theta',\tilde\theta')
\end{align*}


\subsection{Scalability of MCMC. Big-O runtime and bottlenecks}

Our baseline is standard Bayes-MCMC on the original full model.
Let $\tau_{\phi,\theta}$ be the Integrated Autocorrelation Time (IACT) of Bayes-MCMC. If the Effective Sample Size (ESS) is $N$ then we have $T=N\tau_{\phi,\theta}$ MCMC steps. If one Bayes-MCMC step has unit cost then the overall cost is $W_{BM}=T$[time]. This doesn't parallelise.

The SMI posterior is \[
p_{smi,\eta}(\phi,\tilde\theta,\theta|Y,Z)=p_{\eta-pow}(\phi,\tilde\theta|Y,Z)p(\theta|Y,\phi).
\]
If we use the same Bayes-MCMC updates to sample $p_{\eta-pow}(\phi,\tilde\theta|Y,Z)$ then the work sampling $(\phi,\tilde\theta)$ is $W_{Bayes}$. Thin the $\phi$ samples every $\tau_{\phi,\theta}$ steps to get
an ESS about $N$ (rough but reasonable).

Again we use the Bayes-MCMC update for $p(\theta|Y,\phi)$ in SMI. Let $\tau_\theta$ be the IACT. Typically, $\tau_\theta<\tau_{\phi,\theta}$ (lower dimension, proof in special case) so take $\tau_\theta=\tau_{\phi,\theta}$ (conservative).
We run the $\theta$-sampler to equilibrium (initialise $\theta^{(t)}$-run with $\theta^{(t-1)}$). Suppose this takes $K\tau_\theta$ steps ($K\approx 5$ is reasonable).
We have $M_{t}$ threads on each of $M_{p}$ machines and $\theta$-sampling parallelizes with small communication overhead.
A runtime cost for $\theta$-updates equal 1 is conservative, as Bayes-MCMC did $(\phi,\theta)$-updates, so $(\phi,\theta)$-sampling at one $\eta$-value costs about \[
W_{\eta}=W_{BM}(1+K/M_{t}).
\]
Now repeat for $J$ $\eta$-values ($J\approx 20$) using $M_{p}$ machines. Total cost is
\[
W_{smi}=W_{Bayes}(1+K/M_{t}) \times J/M_{p}.
\]
For eg if we assign resources as $M_{p}=J$ and $M_{t}=1$ (use threads for coarse grained stuff) the SMI cost is about $5-10$ times Bayes-MCMC. This reflects our experience.

Finally, we compute and smooth the WAIC across the $J$ runs. This part is fast output-analysis. The ESS must be big enough to get stable WAIC estimates, but WAIC is "nice" to estimate. Very roughly, \[ W_{smi}\simeq 10 W_{Bayes} \] should be achievable without alot of work.

\section{Data Analyses} \label{sec:data_analyses}

\subsection{Simulation study 1: Biased data}

\begin{multicols}{2}
Model:
\begin{align*}
  Z \mid \varphi &\sim N( \varphi, \sigma_z^2 ) \\
  Y \mid \varphi, \theta  &\sim N( \varphi + \theta, \sigma_y^2 )
  % \text{with } & \sigma_z^2, \sigma_y^2 \text{ known}
\end{align*}
with $\sigma_z^2$ and $\sigma_y^2$ (and other $\sigma$'s) known.

Priors:
\begin{align*}
  \varphi &\sim N( 0, \sigma_\varphi^2 ) \\
  \theta &\sim N( 0, \sigma_\theta^2 ) \\
  \tilde\theta &\sim N( 0, \tilde\sigma_\theta^2 )
\end{align*}
\end{multicols}

In our example on the main text, we know the generative parameters: $(\varphi^*,\theta^*, \tilde\theta^*)$. We compare these \textit{true} values with the estimates arising from the $\eta$-smi posterior, for different values of $\eta\in[0,1]$.

% \begin{align*}
%   p( Z \mid \varphi ) &= (2\pi\sigma_z^2)^{-n/2} \exp\{ -\frac{1}{2\sigma_z^2} \sum_{i=1}^n (z_i-\varphi)^2 \} \\
%   p( Y \mid \varphi,  \theta ) &= (2\pi\sigma_y^2)^{-m/2} \exp\{ -\frac{1}{2\sigma_y^2} \sum_{i=1}^m (y_i-\varphi-\theta)^2 \} \\
%   p( \varphi ) &= (2\pi\sigma_\varphi^2)^{-1/2} \exp\{ -\frac{1}{2\sigma_\varphi^2} \varphi^2 \} \\
%   p( \theta ) &= (2\pi\sigma_\theta^2)^{-1/2} \exp\{ -\frac{1}{2\sigma_\theta^2} \theta^2 \} \\
% \end{align*}

\subsubsection{SMI posterior}

First, derive $p(Y \mid \varphi)_{\theta}$ as a function of $\varphi$
\begin{align*}
  p(Y \mid \varphi)_{\theta} &= \frac{1}{p(\varphi)} \; \int  p( Y \mid \varphi,  \theta ) \; p(\varphi,\theta) \;  d\theta \\
  &\propto \exp\{ -\frac{1}{2} [ \varphi^2 ( \frac{m}{m \sigma_\theta^2 + \sigma_y^2 } ) - 2 \varphi (\bar Y \frac{m}{m \sigma_\theta^2 + \sigma_y^2 }) ] \}
\end{align*}

Now we can obtain the $\eta$-smi posterior
\begin{align*}
 p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) &= p(Z \mid \varphi) \; p(Y \mid \varphi, \tilde\theta )^{\eta} p(Y \mid \varphi, \theta ) \; \frac{1}{ p(Z, Y_{\eta} )_{\tilde\theta} \; p(Y \mid \varphi)_{\theta} } p(\varphi, \theta, \tilde\theta) \\
 &\propto \exp\{ -\frac{1}{2} \; [ \varphi^2 ( \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) + \frac{1}{\sigma_\varphi^2} - \frac{m}{\sigma_y^2 + m \sigma_\theta^2} ) - 2 \varphi (\frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \bar Y \frac{m}{ \sigma_y^2 + m \sigma_\theta^2}) + \\
 & \hspace{2cm} \theta^2 (\frac{m}{\sigma_y^2} + \frac{1}{\sigma_\theta^2}) - 2 \theta (\bar Y \frac{m}{\sigma_y^2 }) + \\
 & \hspace{2cm} \tilde\theta^2 ( \eta \frac{m}{\sigma_y^2} + \frac{1}{\tilde\sigma_\theta^2}) - 2 \tilde\theta (\eta \bar Y \frac{m}{\sigma_y^2 }) + \\
 & \hspace{2cm} + 2 \varphi \theta (\frac{m}{\sigma_y^2}) + 2 \varphi \tilde\theta (\eta \frac{m}{\sigma_y^2 }) ] \}
\end{align*}

From here we see that the joint posterior distribution for $(\varphi,\theta,\tilde\theta)$ is a multivariate normal distribution defined as:
\begin{equation} \label{eq:smi_post_5_1}
p_{smi,\eta}(\varphi,\theta,\tilde\theta \mid Z, Y ) = \text{Normal}( \mu, \Sigma ),
\end{equation}

with
\begin{equation*}
  \Sigma = \begin{bmatrix}
  \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \frac{m}{ \sigma_y^2 + m \sigma_\theta^2 } + \frac{1}{\sigma_\varphi^2} & \frac{m}{\sigma_y^2} & \eta \frac{m}{\sigma_y^2 } \\
  \frac{m}{\sigma_y^2} & \frac{m}{\sigma_y^2} + \frac{1}{\sigma_\theta^2} & 0 \\
  \eta \frac{m}{\sigma_y^2 } & 0 &  \eta \frac{m}{\sigma_y^2} + \frac{1}{\tilde\sigma_\theta^2}
\end{bmatrix}^{-1} \text{, and } \mu = \Sigma \begin{bmatrix}
  \frac{n}{\sigma_z^2} + \frac{m}{\sigma_y^2}(1+\eta) - \bar Y \frac{m}{ \sigma_y^2 + m \sigma_\theta^2} \\
  \bar Y \frac{m}{\sigma_y^2 } \\
  \eta \bar Y \frac{m}{\sigma_y^2 }
\end{bmatrix}.
\end{equation*}

The generative parameters described in the main text are as follows
<<biased_data_01, eval=TRUE, echo=TRUE>>=
n=25 # Sample size for Z
m=50 # Sample size for Y

phi = 0 
theta = 1 # bias

sigma_z = 2 # variance for Z
sigma_y = 1 # variance for Y
@

The true bias is $\theta=1$. Assume we have an over-optimistic view of the bias, with prior distribution centered in 0 and relatively small prior variance.
<<biased_data_02, eval=TRUE, echo=FALSE>>=
sigma_phi=Inf # Prior variance for phi
sigma_theta=0.5 # Prior variance for eta
@

<<biased_data_03, eval=TRUE, echo=FALSE>>=
param_names = c('phi','theta','theta_tilde')
param_true = c(phi,theta,theta)

# sequence of eta values in (0,1)
eta_all = seq(0,1,0.01)
@

<<biased_data_04, eval=TRUE, echo=FALSE>>=
set.seed(123)
Z = rnorm( n=n, mean=phi, sd=sigma_z)
Y = rnorm( n=m, mean=phi+theta, sd=sigma_y )
# cat('Z_mean=',mean(Z),'; Y_mean=',mean(Y))

post_eta_all = foreach::foreach(eta = eta_all,.combine='lrcomb', .multicombine=TRUE) %dopar% {
  # eta = 0.01
  # Compute posterior mean and variance
  posterior = aistats2020smi::SMI_post_biased_data( Z=Z, Y=Y, sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta )
  list( t(posterior[[1]]), diag(posterior[[2]]) )
}
# Compute MSE
mse_eta_all = post_eta_all[[2]] + ( post_eta_all[[1]] - t( matrix(param_true,3,length(eta_all)) ) )^2

# Plot posterior vs true value
aistats2020smi::set_ggtheme()
post_plot_all = foreach::foreach( par_i = seq_along(param_names) ) %do% {
  
  p = data.frame( eta=eta_all,
                  post_mean=post_eta_all[[1]][,par_i],
                  post_sd=post_eta_all[[2]][,par_i]^0.5 ) %>%
    ggplot() + 
    geom_line( aes(x=eta,y=post_mean), col='red' ) +
    geom_line( aes(x=eta,y=post_mean+post_sd), col='blue', lty=3 ) +
    geom_line( aes(x=eta,y=post_mean-post_sd), col='blue', lty=3 ) +
    geom_hline(yintercept=param_true[par_i], lty=2) +
    labs(y=paste(param_names[par_i]," posterior",sep=""))
  p
}
p = cowplot::plot_grid(  post_plot_all[[1]], post_plot_all[[2]], post_plot_all[[3]], ncol=1, align='v' )
ggsave( plot=p,
          filename="SMI_biased_posterior_single_dataset.pdf",
          device="pdf", width=10,height=15, units="cm")
@

In Figure~\ref{fig:smi_post_5_1} we show posterior distributions (mean $\pm$ std. dev.) for a randomly generated dataset ($\bar Z=$\Sexpr{round(mean(Z),4)}; $\bar Y=$\Sexpr{round(mean(Y),4)}) using the generative parameters described in the main text. Note that the conventional bayes ($\eta=1$) is the worst estimation for the true parameters $\varphi$ accross all posible candidates $\eta \in [0,1]$.
<<biased_data_05, eval=TRUE, echo=TRUE>>=
# Posterior for conventional bayes eta=1
posterior = SMI_post_biased_data( Z=Z, Y=Y,
                                  sigma_z=sigma_z, sigma_y=sigma_y,
                                  sigma_phi=sigma_phi,
                                  sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta,
                                  eta=1 )
# posterior mean
setNames( c(posterior[[1]]), param_names )
@

\begin{figure}[!ht]
\center
\includegraphics[width=0.5\textwidth]{SMI_biased_posterior_single_dataset}
\caption{Posterior distribution of $\varphi$, $\theta$ and $\tilde\theta$. A black horizontal line shows the true generative value. Mean (solid red line) $\pm$ one std. dev. (dotted blue line) }
  \label{fig:smi_post_5_1}
\end{figure}

\subsubsection{Mean Square Error (MSE)}

From Equation~\ref{eq:smi_post}, we can compute the MSE of estimates arising from the SMI posterior $\varphi$, $\theta$ and , $\tilde\theta$,
\begin{align*}
  MSE(\varphi) &= \Sigma_{[1,1]} + (\mu_{[1]}-\varphi^*)^2\\
  MSE(\theta) &= \Sigma_{[2,2]} + (\mu_{[2]}-\theta^*)^2\\
  MSE(\tilde\theta) &= \Sigma_{[3,3]} + (\mu_{[3]}-\theta^*)^2
\end{align*}

In the first simulation study of the main text we display $MSE(\varphi)$ and $MSE(\theta)$ to demonstrate that we can reach smaller MSE with values of $\eta$ other than 0 and 1. To generate this plots, we produced 1000 synthetic datasets, computed MSE using Eq.~\ref{eq:smi_post} on each one, with a grid of values of $\eta\in[0,1]$. The MSE lines displayed correspond to the \textit{average} MSE across datasets, for each value of $\eta$.

<<biased_data_06, eval=TRUE, echo=FALSE>>=
set.seed(123)
  
# Average Mean Square Error #

# Compute Posterior mean and sd for each iteration
n_iter = 1000
Z = matrix(rnorm( n=n*n_iter, mean=phi, sd=sigma_z),n_iter,n)
Y = matrix(rnorm( n=m*n_iter, mean=phi+theta, sd=sigma_y ),n_iter,m)
post_eta_all_iter = foreach(iter_i = 1:n_iter,.combine='lacomb', .multicombine=TRUE)  %:%
  foreach(eta_i = seq_along(eta_all), .combine='lrcomb', .multicombine=TRUE) %dopar% {
    # eta_i=1
    posterior = aistats2020smi::SMI_post_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    list( t(posterior[[1]]), diag(posterior[[2]]) )
}
# Compute MSE for each iteration
param_true_array = aperm( array(param_true,dim=c(3,length(eta_all),n_iter)) , c(2,1,3) )
MSE_all_iter = post_eta_all_iter[[2]] + (post_eta_all_iter[[1]]-param_true_array)^2

# Average across iterations
post_eta_all_average = list( apply(post_eta_all_iter[[1]],c(1,2),mean),
                             apply(post_eta_all_iter[[2]],c(1,2),mean) + apply(post_eta_all_iter[[1]],c(1,2),var))
MSE_average = apply(MSE_all_iter,c(1,2),mean)

# Plot MSE
aistats2020smi::set_ggtheme()
mse_plot_all = foreach::foreach( par_i = seq_along(param_names) ) %do% {
  # par_i=1
  p = data.frame( eta=eta_all,
                  mse=MSE_average[,par_i] ) %>%
    ggplot() + 
    geom_line( aes(x=eta,y=mse), col='red' ) +
    labs(y=paste("MSE( ",param_names[par_i]," )",sep=""))
  p
}
p = cowplot::plot_grid(  mse_plot_all[[1]], mse_plot_all[[2]], ncol=1, align='v' )
ggsave( plot=p,
        filename="SMI_biased_MSE_average.pdf",
        device="pdf", width=10,height=10, units="cm")

# MSE theta vs theta_tilde
aistats2020smi::set_ggtheme()
p = MSE_average %>%
  `colnames<-`(param_names) %>%
  as.data.frame() %>%
  mutate(eta=eta_all) %>%
  tidyr::pivot_longer(cols=all_of(param_names),names_to='parameter') %>%
  dplyr::filter(parameter %in% c('theta','theta_tilde')) %>%
  ggplot() +
  geom_line( aes(x=eta,y=value,col=parameter) ) +
  coord_cartesian(ylim=c(0.28,0.50)) +
  labs(y="MSE( theta )")
ggsave( plot=p,
        filename="SMI_biased_MSE_average_theta.pdf",
        device="pdf", width=10,height=10, units="cm")
@

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_MSE_average.pdf}
  \caption{Mean Squared Error of the two main parameters under SMI posterior.}
  \label{fig:SMI_biased_MSE_average}
\end{figure}

Here we also show a comparison between $\theta$ and $\tilde\theta$. Figure~\ref{fig:SMI_biased_MSE_average} shows that $\tilde\theta$ is dominated by $\theta$ in MSE. The comparison emphasize the convenience of SMI over power likelihood discussed in section 4.4 of the main text.

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_MSE_average_theta.pdf}
  \caption{Comparison of the bias estimation under SMI (theta) vs powered likelihood (theta tilde)}
  \label{fig:SMI_biased_MSE_average_theta}
\end{figure}

\subsubsection{Expected log pointwise predictive density (elpd)}

The simplicity and conjugacy of this model allow us to derive exact formula for the elpd
\begin{equation*}
  elpd = \int\int p^*(z,y) \log p_{smi,\eta}( z, y \mid Z,Y) dz dy
\end{equation*}
where $p^*$ is the distribution representing the true data-generating process and
\begin{equation*}
  p_{smi,\eta}(z,y \mid Z,Y)=\int\int p(z,y \mid \varphi, \theta) \; p_{smi,\eta}(\varphi,\theta \mid Y,Z)\, d\varphi\,d\theta
\end{equation*}
is a candidate posterior predictive distribution, indexed by $\eta$.

Let $\begin{bmatrix}  a & b \\ b & c \end{bmatrix}=Cov(\varphi,\theta \mid Z,Y)^{-1}$ be the  inverse of the posterior covariance matrix of $(\varphi,\theta)$, and $\begin{bmatrix}  d \\ e \end{bmatrix}=E(\varphi,\theta \mid Z,Y)$ the posterior means.

Following straightforward Gaussian completion we can show that the joint posterior distribution for $\varphi$, $\theta$, and new data $z_0$ and $y_0$ is:
\begin{align*}
  p_{smi,\eta}(z_0,y_0,\varphi,\theta|Z,Y) &\propto p(z_0,y_0|\varphi,\theta) \; p_{smi,\eta}(\varphi,\theta|Z,Y) \\
  &\propto \exp\{ -\frac{1}{2} \; [ z_0^2 (\frac{1}{\sigma_z^2}) + y_0^2 (\frac{1}{\sigma_y^2}) + \varphi^2 (a+\frac{1}{\sigma_z^2}+\frac{1}{\sigma_y^2}) + \theta^2 (c + \frac{1}{\sigma_y^2} ) + \\
  & \hspace{2cm} - 2 z_0 \varphi (\frac{1}{\sigma_z^2}) - 2 y_0 \varphi (\frac{1}{\sigma_y^2}) - 2 y_0 \theta (\frac{1}{\sigma_y^2}) + 2 \varphi \theta (b+\frac{1}{\sigma_y^2}) \\
  & \hspace{2cm} -2 \varphi (a d + b e) -2 \theta (b d + c e ) ] \}
\end{align*}

So we have
\begin{equation} \label{eq:smi_post_pred_5_1}
p_{smi,\eta}(z_0,y_0,\varphi,\theta \mid Z, Y ) = \text{Normal}( \mu, \Sigma ),
\end{equation}

with
\begin{equation*}
  \Sigma = \begin{bmatrix}
  \frac{1}{\sigma_z^2} & 0 & -\frac{1}{\sigma_z^2} & 0 \\
  0 & \frac{1}{\sigma_y^2} & -\frac{1}{\sigma_y^2} & -\frac{1}{\sigma_y^2} \\
  -\frac{1}{\sigma_z^2} & -\frac{1}{\sigma_y^2} & a+\frac{1}{\sigma_z^2}+\frac{1}{\sigma_y^2} & b+\frac{1}{\sigma_y^2} \\
  0 & -\frac{1}{\sigma_y^2} & b+\frac{1}{\sigma_y^2} & c + \frac{1}{\sigma_y^2}
\end{bmatrix}^{-1} \text{, and } \mu = \Sigma \begin{bmatrix}
  0 \\
  0 \\
  a d + b e \\
  b d + c e
\end{bmatrix}.
\end{equation*}

We know the true generative values $\varphi^*$ and $\theta^*$, so we can compute $elpd$ using Monte Carlo samples from the true generative distribution $p^*$ and evaluate this values in the log-density of the bivariate normal $(z_0,y_0|Z,Y)$ from Equation~\ref{eq:smi_post_pred_5_1}.

<<biased_data_07, eval=TRUE, echo=FALSE>>=
# ELPD approximation via Monte Carlo
set.seed(0)
n_new = 1000

# generate data from the ground-truth distribution
Z_new = matrix( rnorm( n=n_iter*n_new, mean=phi, sd=sigma_z), n_iter, n_new )
Y_new = matrix( rnorm( n=n_iter*n_new, mean=phi+theta, sd=sigma_y), n_iter, n_new )

log_pred_eta_all_iter = foreach(iter_i = 1:n_iter, .combine='acomb', .multicombine=TRUE) %:%
  foreach( eta_i = seq_along(eta_all), .combine=rbind ) %dopar% {
    # iter_i=1
    # new_i = 1
    # eta_i=1
    # posterior = aistats2020smi::SMI_post_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    predictive = aistats2020smi::SMI_pred_biased_data( Z=Z[iter_i,], Y=Y[iter_i,], sigma_z=sigma_z, sigma_y=sigma_y, sigma_phi=sigma_phi, sigma_theta=sigma_theta, sigma_theta_tilde=sigma_theta, eta=eta_all[eta_i] )
    mvtnorm::dmvnorm( x=cbind(Z_new[iter_i,],Y_new[iter_i,]), mean=predictive[[1]] , sigma=predictive[[2]], log=TRUE )
}
# average to get elpd
elpd_eta_all = apply(log_pred_eta_all_iter,1,mean)

# Plot ELPD
aistats2020smi::set_ggtheme()
p = data.frame(eta=eta_all, elpd=elpd_eta_all) %>%
  ggplot() +
  geom_line( aes(x=eta,y=-elpd),col='red' ) +
  labs(y="- elpd(z,y)")
ggsave( plot=p,
          filename="SMI_biased_elpd.pdf",
          device="pdf", width=10,height=7, units="cm")
@

In Figure~\ref{fig:SMI_biased_elpd} we show the Monte Carlo estimation of the $elpd$. We select the optimal $\eta$ as the value that maximise the $elpd$. To generate this plot, we produced 1000 synthetic datasets, computed $elpd$ on each one (using Monte Carlo), with a grid of values of $\eta\in[0,1]$. The $elpd$ line correspond to the \textit{average} elpd across datasets, for each value of $\eta$.

\begin{figure}[!ht]
  \center
  \includegraphics[width=0.5\textwidth]{SMI_biased_elpd.pdf}
  \caption{ELPD under SMI posterior.}
  \label{fig:SMI_biased_elpd}
\end{figure}

\subsection{Experiments on Bayesian Multiple Imputation}\label{sec:bayes_impute}

Bla

\subsection{Agricultural practices in early civilizations} \label{sec:agric_analysis}

The aim of the study, described in detail in \cite{Styring2017}, is to provide statistical evidence about a specific agricultural practice of the first urban centres in northern Mesopotamia.

The hypothesis is that increased agricultural production to support growing urban populations was achieved by cultivation of larger areas of land, entailing lower manure/midden inputs per unit area. This practice is known as \emph{extensification}.

Our contribution goes into extending the methods used to perform Bayesian analysis in this adverse scenario of model misspecification and big missing data.

\subsubsection{ Data }

The data consists of measurements of nitrogen and carbon isotopes for a collection of crop remains. There are two datasets: archaeological and calibration, which we will denote by $\mathcal{M}$ and $\mathcal{C}$, respectively. First, the \emph{Archaeological} dataset $\mathcal{M}$, consists of data gathered from excavations of antique crop sites in the region of Mesopotamia. Second, the \emph{Calibration} dataset, $\mathcal{C}$, which was gathered in a controlled experimental setting in recent years. Further characteristics and description of variables on each dataset can be found in the statistical supplement to \cite{Styring2017}.

\subsubsection{ The model } \label{sec:agric_model}

We preserved the model stated in \cite{Styring2017}. The main goal of this study is to is to provide statistical evidence for the hypothesis of \emph{extensification} in the ancient urban sites. This hypothesis can be condensed in analysing the strength of the effect of the site size $S_i$ on the corresponding manure level $M_i$ for the records in the archaeological data, $\{i \in \mathcal{M}\}$. The more negative the estimated effect, the stronger the evidence is supporting the extensification hypothesis.

However, there is no available information about the Manuring levels in the ancient dataset. This is addressed by using a \emph{Data augmentation} perspective, and consider that all the corresponding values of the manure level in the Archaeological data are missing. We will denote this values as denote these unobserved manure levels as $M_{\mathcal{M}}=\{M_i; i \in \mathcal{M}\}$.

The model consists of a two-module model, which integrates archaeological and calibration data so the missing manuring levels can be inferred. The first module is a Proportional Odds model for the missing Manure Levels ($M_{\mathcal{M}}$) in the archaeological data. The second module consists of a linear Gaussian model for the Nitrogen level of the crops ($Z$), which is applicable to both datasets.

 The setting here is similar to the experiment in Section~\ref{sec:exp_1} above, but incorporating more covariates into both modules ($S$: size of crop field; $P$: indicator of historical context; $R$: rainfall level; $C$ category of crop species). Figure~\ref{fig:graph_model} depicts the graphical representation of the model.


\bibliographystyle{apalike} %use the apalike bibliography style
\bibliography{Mendeley} % Mendeley bibliography

\end{document}
